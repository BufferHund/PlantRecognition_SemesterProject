{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7880,"databundleVersionId":862031,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Experiment 1 – Small-Scale Dataset Architecture Benchmarking\n\n#### This experiment evaluates how different convolutional neural network architectures—CustomCNN-S, ResNet-18, and ConvNeXt-Tiny—perform on the small Plant Seedlings dataset. By comparing models trained from scratch versus those with ImageNet pretraining, it highlights the importance of model capacity, inductive biases, and transfer learning in data-constrained settings","metadata":{}},{"cell_type":"markdown","source":"###  1. Imports and Environment Setup\n#### This first block imports all necessary libraries and sets up the environment. It includes a setting to force unbuffered output, which is helpful for seeing logs in real-time within notebooks or containerized environments.","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# Plant Seedlings — One-Block Training with Rich Logging (Fixed CAM)\n# Backbones: customcnn / resnet18 / convnext_tiny\n# Features: OneCycle, Mixup/CutMix, per-class metrics, LR track,\n#           t-SNE features dump, Grad-CAM samples, resource profile\n# ============================================================\n\nimport os, math, random, time, csv, json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom collections import Counter\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms as T, models\nfrom tqdm.notebook import tqdm # Use notebook-friendly tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:14.217983Z","iopub.execute_input":"2025-09-29T15:41:14.218314Z","iopub.status.idle":"2025-09-29T15:41:14.223722Z","shell.execute_reply.started":"2025-09-29T15:41:14.218294Z","shell.execute_reply":"2025-09-29T15:41:14.222808Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Configuration\n#### All hyperparameters and configuration settings are centralized here. It is easy to experiment with different settings like batch size, model dimensions, learning rates here.","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass CFG:\n    project_name: str = \"seedlings-3backbones\"\n    # === choose model & pretrained ===\n    model_name: str = \"customcnn\"    # \"customcnn\" | \"resnet18\" | \"convnext_tiny\"\n    use_pretrained: bool = False     # True (ImageNet) or False (scratch)\n    # ================================\n    seed: int = 42\n    num_workers: int = 0             # 0 is more stable on Kaggle\n    epochs: int = 50                 # Recommended 50 for scratch, 30 for pretrained\n    img_size: int = 224\n    batch_size: int = 128\n    weight_decay: float = 1e-2\n    label_smoothing: float = 0.1\n    debug: bool = False              # Use a small subset of data for quick tests\n\n    # LR scheduler: OneCycle\n    use_onecycle: bool = True\n    max_lr: float = 3e-3\n    onecycle_pct_start: float = 0.15\n    onecycle_div_factor: float = 25.0\n    onecycle_final_div: float = 1e4\n    # (Alternative Cosine scheduler, not used by default)\n    lr: float = 1e-3\n    min_lr: float = 1e-6\n    warmup_epochs: int = 2\n\n    # Mixup / CutMix (disabled during last `aug_off_frac` of training)\n    use_mixup_cutmix: bool = True\n    mixup_alpha: float = 0.1\n    cutmix_alpha: float = 0.1\n    aug_off_frac: float = 0.2\n\n    # Optional fine-tune @288px\n    do_finetune_288: bool = True\n    finetune_epochs: int = 3\n    finetune_lr: float = 5e-5\n\n    # Inference\n    do_tta: bool = True              # Use Test-Time Augmentation (horizontal flip)\n    save_dir: str = \"/kaggle/working\"\n    num_classes: int = None          # Will be set automatically based on data\n\n    # ==== Rich logging for analysis ====\n    log_dir: str = \"/kaggle/working/metrics\"\n    enable_per_class: bool = True    # Log per-class metrics & confusion matrix\n    enable_lr_track: bool = True     # Record learning rate per step\n    enable_feature_dump: bool = True # Export validation set features for t-SNE\n    enable_cam: bool = True          # Export Grad-CAM samples\n    cam_samples: int = 12            # Number of CAM samples to export\n    measure_resource: bool = True    # Profile model parameters/throughput/memory\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:14.224920Z","iopub.execute_input":"2025-09-29T15:41:14.225123Z","iopub.status.idle":"2025-09-29T15:41:14.244211Z","shell.execute_reply.started":"2025-09-29T15:41:14.225108Z","shell.execute_reply":"2025-09-29T15:41:14.243611Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Logging and Utilities\n#### This section contains functions for setting up logging and other miscellaneous utilities. Functions for setting random seeds and formatting bytes are also included.","metadata":{}},{"cell_type":"code","source":"def set_seed(s=42):\n    \"\"\"Sets the seed for random, numpy, and torch for reproducibility.\"\"\"\n    random.seed(s)\n    np.random.seed(s)\n    torch.manual_seed(s)\n    torch.cuda.manual_seed_all(s)\n    torch.backends.cudnn.benchmark = True\nset_seed(cfg.seed)\n\ndef fmt_time(sec):\n    \"\"\"Formats seconds into a MM:SS string.\"\"\"\n    m = int(sec // 60)\n    s = int(sec % 60)\n    return f\"{m:02d}:{s:02d}\"\n\n# --- Data paths ---\nKAGGLE_INPUT = Path(\"/kaggle/input\")\nROOT = KAGGLE_INPUT / \"plant-seedlings-classification\"\nTRAIN = ROOT / \"train\"\nTEST = ROOT / \"test\"\nSAMPLE = ROOT / \"sample_submission.csv\"\n\n# A quick check to ensure the dataset is available\nassert TRAIN.exists(), \"Please add the `Plant Seedlings Classification` dataset from Kaggle.\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:14.245125Z","iopub.execute_input":"2025-09-29T15:41:14.245317Z","iopub.status.idle":"2025-09-29T15:41:14.269787Z","shell.execute_reply.started":"2025-09-29T15:41:14.245304Z","shell.execute_reply":"2025-09-29T15:41:14.269185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Create Datasets and Dataloaders\n#### This block executes the data preparation steps: splitting the data, creating Dataset and DataLoader instances, and calculating class weights to handle imbalance.","metadata":{}},{"cell_type":"code","source":"class DS(Dataset):\n    \"\"\"Custom PyTorch Dataset for loading seedling images.\"\"\"\n    def __init__(self, items, tfm):\n        self.items, self.tfm = items, tfm\n    def __len__(self):\n        return len(self.items)\n    def __getitem__(self, i):\n        p, y = self.items[i]\n        img = Image.open(p).convert(\"RGB\")\n        x = self.tfm(img)\n        return x, y, str(p)\n\ndef build_split(val_ratio=0.1):\n    \"\"\"Scans the training data and creates stratified train/validation splits.\"\"\"\n    classes = sorted([d.name for d in TRAIN.iterdir() if d.is_dir()])\n    c2i = {c: i for i, c in enumerate(classes)}\n    items = []\n    for c in classes:\n        for p in (TRAIN / c).glob(\"*.*\"):\n            if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"]:\n                items.append((p, c2i[c]))\n    y = np.array([b for _, b in items])\n    idx = np.arange(len(items))\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=cfg.seed)\n    tr, va = next(sss.split(idx, y))\n    return classes, [items[i] for i in tr], [items[i] for i in va]\n\ndef get_tfms(size, strong=True):\n    \"\"\"Returns image transformations for training and validation.\"\"\"\n    if strong:\n        # Strong augmentations for the main training phase\n        train = T.Compose([\n            T.Resize(int(size * 1.14)),\n            T.RandomResizedCrop(size, scale=(0.8, 1.0), ratio=(3/4, 4/3)),\n            T.RandomHorizontalFlip(0.5),\n            T.RandAugment(2, 9),\n            T.ToTensor(),\n            T.RandomErasing(p=0.1),\n            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n    else:\n        # Weaker augmentations for the final epochs or fine-tuning\n        train = T.Compose([\n            T.Resize(int(size * 1.14)),\n            T.RandomResizedCrop(size, scale=(0.9, 1.0), ratio=(3/4, 4/3)),\n            T.RandomHorizontalFlip(0.5),\n            T.ToTensor(),\n            T.RandomErasing(p=0.1),\n            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n    # Validation transforms (no augmentation, just resize and crop)\n    valid = T.Compose([\n        T.Resize(int(size * 1.14)),\n        T.CenterCrop(size),\n        T.ToTensor(),\n        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ])\n    return train, valid\n\n# Create the train/validation split\nclasses, train_items, valid_items = build_split(0.1)\ncfg.num_classes = len(classes)\nif cfg.debug: # If debugging, use a smaller subset\n    train_items = train_items[:1000]\n    valid_items = valid_items[:200]\n\nprint(f\"Dataset: seedlings | classes={cfg.num_classes} | train={len(train_items)} | valid={len(valid_items)}\")\nprint(f\"Classes: {classes}\")\n\n# Create transforms and datasets\ntrain_tfm, valid_tfm = get_tfms(cfg.img_size, strong=True)\ntrain_ds = DS(train_items, train_tfm)\nvalid_ds = DS(valid_items, valid_tfm)\n\n# Create dataloaders\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n                          num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False,\n                          num_workers=cfg.num_workers, pin_memory=True)\n\n# Calculate soft class weights to handle class imbalance\nfreq = Counter([y for _, y in train_items])\ngamma = 0.5 # Smoothing factor\nw = torch.tensor([(1.0 / freq[i])**gamma for i in range(cfg.num_classes)], dtype=torch.float)\nw = (w / w.mean())\nprint(f\"Calculated class weights: {w.numpy().round(2)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:14.270415Z","iopub.execute_input":"2025-09-29T15:41:14.270619Z","iopub.status.idle":"2025-09-29T15:41:14.547274Z","shell.execute_reply.started":"2025-09-29T15:41:14.270595Z","shell.execute_reply":"2025-09-29T15:41:14.546536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Model Definition\n#### This section defines the neural network architectures. It includes a custom CNN built from scratch and wrapper functions to load popular pretrained models like ResNet-18 and ConvNeXt-Tiny from torchvision.","metadata":{}},{"cell_type":"code","source":"class SqueezeExcite(nn.Module):\n    \"\"\"A simple Squeeze-and-Excitation block.\"\"\"\n    def __init__(self, c, r=8):\n        super().__init__()\n        self.fc1 = nn.Conv2d(c, c // r, 1)\n        self.fc2 = nn.Conv2d(c // r, c, 1)\n    def forward(self, x):\n        w = F.adaptive_avg_pool2d(x, 1)\n        w = F.silu(self.fc1(w))\n        w = torch.sigmoid(self.fc2(w))\n        return x * w\n\nclass DWSeparable(nn.Module):\n    \"\"\"A Depthwise Separable Convolution block with optional Squeeze-Excite.\"\"\"\n    def __init__(self, in_c, out_c, k=3, s=1, p=1, se=True, drop=0.1):\n        super().__init__()\n        self.dw = nn.Conv2d(in_c, in_c, k, s, p, groups=in_c, bias=False)\n        self.pw = nn.Conv2d(in_c, out_c, 1, 1, 0, bias=False)\n        self.bn = nn.BatchNorm2d(out_c)\n        self.act = nn.SiLU()\n        self.se = SqueezeExcite(out_c) if se else nn.Identity()\n        self.dropout = nn.Dropout2d(drop) if drop > 0 else nn.Identity()\n    def forward(self, x):\n        x = self.dw(x); x = self.pw(x); x = self.bn(x); x = self.act(x)\n        x = self.se(x); x = self.dropout(x)\n        return x\n\nclass CustomCNNS(nn.Module):\n    \"\"\"A custom lightweight CNN with ~1.3M parameters.\"\"\"\n    def __init__(self, num_classes):\n        super().__init__()\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(64), nn.SiLU(),\n        )\n        self.stage1 = nn.Sequential(DWSeparable(64, 64), DWSeparable(64, 64))\n        self.down1 = nn.Conv2d(64, 128, 3, 2, 1, bias=False)\n        self.stage2 = nn.Sequential(DWSeparable(128, 128, drop=0.15), DWSeparable(128, 128, drop=0.15))\n        self.down2 = nn.Conv2d(128, 256, 3, 2, 1, bias=False)\n        self.stage3 = nn.Sequential(DWSeparable(256, 256, drop=0.2), DWSeparable(256, 256, drop=0.2))\n        self.head = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n            nn.Linear(256, num_classes)\n        )\n        # Initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            if isinstance(m, nn.Linear):\n                nn.init.trunc_normal_(m.weight, std=0.02)\n                if m.bias is not None: nn.init.zeros_(m.bias)\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.stage1(x); x = self.down1(x)\n        x = self.stage2(x); x = self.down2(x)\n        x = self.stage3(x)\n        return self.head(x)\n\ndef _try_download_weights(build_fn_newapi, build_fn_legacy, name):\n    \"\"\"Helper to download pretrained weights, compatible with old/new torchvision APIs.\"\"\"\n    os.environ.setdefault(\"TORCH_HOME\", \"/kaggle/working/.cache/torch\")\n    os.makedirs(os.environ[\"TORCH_HOME\"], exist_ok=True)\n    try:\n        m = build_fn_newapi()\n        print(f\"[weights] {name}: new API loaded.\")\n        return m\n    except Exception as e1:\n        print(f\"[weights] {name}: new API failed -> {e1}\")\n        if build_fn_legacy is None: return None\n        try:\n            m = build_fn_legacy()\n            print(f\"[weights] {name}: legacy pretrained=True loaded.\")\n            return m\n        except Exception as e2:\n            print(f\"[weights] {name}: legacy failed -> {e2}\")\n            return None\n\ndef build_resnet18(num_classes, pretrained=True):\n    \"\"\"Builds a ResNet-18 model with a modified classifier head.\"\"\"\n    if not pretrained:\n        m = models.resnet18(weights=None)\n    else:\n        def newapi():\n            from torchvision.models import ResNet18_Weights\n            return models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n        def legacy(): return models.resnet18(pretrained=True)\n        m = _try_download_weights(newapi, legacy, \"resnet18\") or models.resnet18(weights=None)\n    m.fc = nn.Linear(m.fc.in_features, num_classes)\n    return m\n\ndef build_convnext_tiny(num_classes, pretrained=True):\n    \"\"\"Builds a ConvNeXt-Tiny model with a modified classifier head.\"\"\"\n    if not pretrained:\n        m = models.convnext_tiny(weights=None)\n    else:\n        def newapi():\n            from torchvision.models import ConvNeXt_Tiny_Weights\n            return models.convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n        def legacy(): return models.convnext_tiny(pretrained=True)\n        m = _try_download_weights(newapi, legacy, \"convnext_tiny\") or models.convnext_tiny(weights=None)\n    in_f = m.classifier[2].in_features\n    m.classifier[2] = nn.Linear(in_f, num_classes)\n    return m\n\ndef build_model(name: str, num_classes: int, use_pretrained: bool):\n    \"\"\"A factory function to build the selected model.\"\"\"\n    name = name.lower()\n    if name == \"customcnn\":     return CustomCNNS(num_classes)\n    if name == \"resnet18\":      return build_resnet18(num_classes, pretrained=use_pretrained)\n    if name == \"convnext_tiny\": return build_convnext_tiny(num_classes, pretrained=use_pretrained)\n    raise ValueError(f\"Unknown model: {name}\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = build_model(cfg.model_name, cfg.num_classes, cfg.use_pretrained).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\ncriterion = nn.CrossEntropyLoss(weight=w.to(device), label_smoothing=cfg.label_smoothing)\nscaler = torch.amp.GradScaler('cuda')\n\nif cfg.use_onecycle:\n    steps_per_epoch = len(train_loader)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=cfg.max_lr, epochs=cfg.epochs, steps_per_epoch=steps_per_epoch,\n        pct_start=cfg.onecycle_pct_start, div_factor=cfg.onecycle_div_factor,\n        final_div_factor=cfg.onecycle_final_div, anneal_strategy='cos'\n    )\nelse: # Fallback to a cosine scheduler with warmup\n    total_steps = len(train_loader) * cfg.epochs\n    warmup_steps = max(1, len(train_loader) * cfg.warmup_epochs)\n    def lr_lambda(step):\n        if step < warmup_steps: return step / max(1, warmup_steps)\n        prog = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n        return max(cfg.min_lr / cfg.lr, 0.5 * (1 + math.cos(math.pi * prog)))\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:14.548524Z","iopub.execute_input":"2025-09-29T15:41:14.548757Z","iopub.status.idle":"2025-09-29T15:41:14.752851Z","shell.execute_reply.started":"2025-09-29T15:41:14.548741Z","shell.execute_reply":"2025-09-29T15:41:14.752251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6. Logging and Resource Profiling\n#### We set up a simple CSVLogger to save training metrics and perform a quick resource profile to measure model parameters, latency, and throughput.","metadata":{}},{"cell_type":"code","source":"os.makedirs(cfg.log_dir, exist_ok=True)\nclass CSVLogger:\n    \"\"\"A simple CSV logger for training metrics.\"\"\"\n    def __init__(self, path, header):\n        self.path = path\n        self.header = header\n        if not os.path.exists(path):\n            with open(path, \"w\", newline=\"\") as f:\n                csv.writer(f).writerow(header)\n    def log(self, row_dict):\n        with open(self.path, \"a\", newline=\"\") as f:\n            csv.writer(f).writerow([row_dict.get(h, \"\") for h in self.header])\n\ntrainlog = CSVLogger(\n    os.path.join(cfg.log_dir, \"train_curve.csv\"),\n    [\"timestamp\", \"epoch\", \"phase\", \"loss\", \"acc\", \"f1\", \"lr\", \"lam\", \"time_sec\", \"model\", \"pretrained\"]\n)\n\n# --- Resource quick profile ---\nif cfg.measure_resource:\n    with torch.no_grad():\n        dummy = torch.randn(1, 3, cfg.img_size, cfg.img_size).to(device)\n        params_m = sum(p.numel() for p in model.parameters()) / 1e6\n        \n        # Warm-up pass\n        t0 = time.time()\n        _ = model(dummy)\n        if torch.cuda.is_available(): torch.cuda.synchronize()\n        t1 = time.time()\n        warm_latency = t1 - t0\n\n        # Throughput test\n        rep = 8\n        if torch.cuda.is_available(): torch.cuda.synchronize()\n        t0 = time.time()\n        for _ in range(rep): _ = model(dummy)\n        if torch.cuda.is_available(): torch.cuda.synchronize()\n        t1 = time.time()\n        thr = rep / (t1 - t0 + 1e-9)\n\n        # Save results\n        resource_data = {\n            \"model\": cfg.model_name, \"pretrained\": bool(cfg.use_pretrained),\n            \"params_M\": round(params_m, 3), \"single_pass_s\": round(warm_latency, 4),\n            \"throughput_iter_per_s\": round(thr, 2), \"img_size\": cfg.img_size,\n        }\n        with open(os.path.join(cfg.log_dir, \"resource.json\"), \"w\") as f:\n            json.dump(resource_data, f, indent=2)\n        print(f\"[Resource] Model: {cfg.model_name} | Params: {params_m:.2f}M | Throughput: {thr:.1f} iter/s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:14.753565Z","iopub.execute_input":"2025-09-29T15:41:14.753810Z","iopub.status.idle":"2025-09-29T15:41:15.599006Z","shell.execute_reply.started":"2025-09-29T15:41:14.753788Z","shell.execute_reply":"2025-09-29T15:41:15.598364Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Mixup & CutMix Implementation\n#### This block contains the helper functions for applying Mixup and CutMix, two powerful data augmentation techniques that mix images and their labels to improve generalization.","metadata":{}},{"cell_type":"code","source":"def rand_bbox(W, H, lam):\n    \"\"\"Generates a random bounding box for CutMix.\"\"\"\n    cut_rat = math.sqrt(1. - lam)\n    cw, ch = int(W * cut_rat), int(H * cut_rat)\n    cx, cy = random.randint(0, W), random.randint(0, H)\n    x1, y1 = np.clip(cx - cw // 2, 0, W), np.clip(cy - ch // 2, 0, H)\n    x2, y2 = np.clip(cx + cw // 2, 0, W), np.clip(cy + ch // 2, 0, H)\n    return x1, y1, x2, y2\n\ndef apply_mixup(x, y, alpha):\n    \"\"\"Applies Mixup augmentation.\"\"\"\n    if alpha <= 0: return x, y, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    idx = torch.randperm(x.size(0), device=x.device)\n    return lam * x + (1 - lam) * x[idx], y, y[idx], lam\n\ndef apply_cutmix(x, y, alpha):\n    \"\"\"Applies CutMix augmentation.\"\"\"\n    if alpha <= 0: return x, y, None, 1.0\n    lam = np.random.beta(alpha, alpha)\n    bs, C, H, W = x.size()\n    idx = torch.randperm(bs, device=x.device)\n    x1, y1, x2, y2 = rand_bbox(W, H, lam)\n    xx = x.clone()\n    xx[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))\n    return xx, y, y[idx], lam\n\ndef criterion_mc(crit, logits, y1, y2, lam):\n    \"\"\"Loss function for Mixup/CutMix. It's a linear combination of two losses.\"\"\"\n    if y2 is None: return crit(logits, y1)\n    return lam * crit(logits, y1) + (1 - lam) * crit(logits, y2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:15.599717Z","iopub.execute_input":"2025-09-29T15:41:15.599965Z","iopub.status.idle":"2025-09-29T15:41:15.607592Z","shell.execute_reply.started":"2025-09-29T15:41:15.599948Z","shell.execute_reply":"2025-09-29T15:41:15.607036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8.Validation Function\n#### The validate function evaluates the model on the validation set. It's decorated with @torch.no_grad() for efficiency. It can also generate and save detailed classification reports and confusion matrices for deeper analysis.","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef validate(return_details: bool = False, epoch_idx: int = 0):\n    \"\"\"Evaluates the model on the validation set.\"\"\"\n    model.eval()\n    loss_sum = acc_sum = n = 0\n    all_logits = []\n    all_true = []\n    \n    for x, y, _ in valid_loader:\n        x, y = x.to(device), y.to(device)\n        with torch.amp.autocast('cuda'):\n            lo = model(x)\n            ls = criterion(lo, y)\n        \n        bs = x.size(0)\n        loss_sum += ls.item() * bs\n        acc_sum += (lo.argmax(1) == y).float().sum().item()\n        n += bs\n        all_logits.append(lo.cpu())\n        all_true.append(y.cpu())\n        \n    logits = torch.cat(all_logits, 0).numpy()\n    true = torch.cat(all_true, 0).numpy()\n    pred = logits.argmax(1)\n    \n    f1 = f1_score(true, pred, average=\"macro\")\n    val_loss, val_acc = loss_sum / n, acc_sum / n\n\n    details = {}\n    if return_details and cfg.enable_per_class:\n        # Generate and save detailed reports\n        rep = classification_report(true, pred, target_names=classes, output_dict=True, zero_division=0)\n        cm = confusion_matrix(true, pred, labels=list(range(cfg.num_classes)))\n        pd.DataFrame(rep).to_csv(os.path.join(cfg.log_dir, f\"per_class_report_epoch{epoch_idx:03d}.csv\"))\n        pd.DataFrame(cm, index=classes, columns=classes).to_csv(os.path.join(cfg.log_dir, f\"confusion_matrix_epoch{epoch_idx:03d}.csv\"))\n        details[\"report_dict\"] = rep\n        details[\"confusion\"] = cm.tolist()\n        \n    return (val_loss, val_acc, f1) if not return_details else (val_loss, val_acc, f1, details)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:15.608361Z","iopub.execute_input":"2025-09-29T15:41:15.608624Z","iopub.status.idle":"2025-09-29T15:41:15.629390Z","shell.execute_reply.started":"2025-09-29T15:41:15.608599Z","shell.execute_reply":"2025-09-29T15:41:15.628898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9. Main Training Loop\n#### This is the core of the experiment. It iterates through epochs, applies augmentations, computes loss, performs backpropagation, and updates model weights. It also logs metrics and saves the best-performing model checkpoint based on validation accuracy.","metadata":{}},{"cell_type":"code","source":"best_acc = 0.0\nbest_path = Path(cfg.save_dir) / f\"{cfg.project_name}_{cfg.model_name}_{'pt' if cfg.use_pretrained else 'scratch'}_best.pt\"\n\nfor epoch in range(cfg.epochs):\n    model.train()\n    # Check if we are in the final phase to turn off strong augmentations\n    late_phase = (epoch >= int(cfg.epochs * (1 - cfg.aug_off_frac)))\n    if late_phase:\n        train_tfm, _ = get_tfms(cfg.img_size, strong=False)\n        train_ds.tfm = train_tfm\n\n    loss_sum = acc_sum = n = 0\n    t0 = time.time()\n    \n    pbar = tqdm(train_loader, desc=f\"[{cfg.model_name}] Epoch {epoch+1}/{cfg.epochs}\")\n    for x, y, _ in pbar:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad(set_to_none=True)\n\n        # Apply Mixup/CutMix\n        y2, lam = None, 1.0\n        if cfg.use_mixup_cutmix and not late_phase:\n            if random.random() < 0.5 and cfg.mixup_alpha > 0:\n                x, y, y2, lam = apply_mixup(x, y, cfg.mixup_alpha)\n            elif cfg.cutmix_alpha > 0:\n                x, y, y2, lam = apply_cutmix(x, y, cfg.cutmix_alpha)\n\n        # Forward pass with mixed precision\n        with torch.amp.autocast('cuda'):\n            lo = model(x)\n            ls = criterion_mc(criterion, lo, y, y2, lam)\n\n        # Backward pass\n        scaler.scale(ls).backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n        # Update running metrics\n        bs = x.size(0)\n        loss_sum += ls.item() * bs\n        acc_sum += (lo.argmax(1) == y).float().sum().item()\n        n += bs\n        pbar.set_postfix(loss=f\"{loss_sum/n:.4f}\", acc=f\"{acc_sum/n:.4f}\")\n\n    # --- End of Epoch ---\n    tr_loss, tr_acc = loss_sum / n, acc_sum / n\n    va_loss, va_acc, va_f1, _ = validate(return_details=True, epoch_idx=epoch + 1)\n    \n    print(f\"[{cfg.model_name}] [{epoch+1:02d}/{cfg.epochs}] train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n          f\"val_loss={va_loss:.4f} acc={va_acc:.4f} f1={va_f1:.4f} | time={fmt_time(time.time()-t0)}\")\n\n    # Log metrics to CSV\n    log_payload = {\"timestamp\": datetime.now().isoformat(timespec=\"seconds\"), \"epoch\": epoch + 1,\n                   \"time_sec\": f\"{int(time.time()-t0)}\", \"model\": cfg.model_name, \"pretrained\": int(cfg.use_pretrained)}\n    trainlog.log({**log_payload, \"phase\": \"train\", \"loss\": f\"{tr_loss:.6f}\", \"acc\": f\"{tr_acc:.6f}\", \"lr\": f\"{optimizer.param_groups[0]['lr']:.6e}\"})\n    trainlog.log({**log_payload, \"phase\": \"valid\", \"loss\": f\"{va_loss:.6f}\", \"acc\": f\"{va_acc:.6f}\", \"f1\": f\"{va_f1:.6f}\"})\n\n    # Save best model\n    if va_acc > best_acc:\n        best_acc = va_acc\n        torch.save({\"model\": model.state_dict(), \"classes\": classes}, best_path)\n        print(f\"  -> Saved new best (val_acc={best_acc:.4f}) to {best_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T15:41:15.631052Z","iopub.execute_input":"2025-09-29T15:41:15.631431Z","execution_failed":"2025-09-29T15:41:39.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 10. Optional Fine-Tuning (288px) \n#### After the main training is complete, this optional block loads the best model and fine-tunes it for a few more epochs on a higher image resolution (288x288). This can often lead to a significant performance boost.","metadata":{}},{"cell_type":"code","source":"if cfg.do_finetune_288:\n    print(f\"\\n[FT] Starting fine-tuning at 288px for {cfg.finetune_epochs} epochs...\")\n    # Load the best model from the previous stage\n    ckpt = torch.load(best_path, map_location=\"cpu\")\n    model.load_state_dict(ckpt[\"model\"])\n    model.to(device)\n\n    # Update datasets with new image size and weaker augmentations\n    ft_train_tfm, ft_valid_tfm = get_tfms(288, strong=False)\n    train_ds.tfm = ft_train_tfm\n    valid_ds.tfm = ft_valid_tfm\n    \n    # Use a smaller batch size for the larger image resolution to avoid memory issues\n    ft_batch_size = max(64, cfg.batch_size // 2)\n    train_loader = DataLoader(train_ds, batch_size=ft_batch_size, shuffle=True,\n                              num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_ds, batch_size=ft_batch_size, shuffle=False,\n                              num_workers=cfg.num_workers, pin_memory=True)\n                              \n    # Use a new optimizer with a small learning rate for fine-tuning\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.finetune_lr, weight_decay=cfg.weight_decay)\n    scaler = torch.amp.GradScaler('cuda')\n\n    for e in range(cfg.finetune_epochs):\n        model.train()\n        t0 = time.time()\n        loss_sum = acc_sum = n = 0\n        pbar = tqdm(train_loader, desc=f\"FT Epoch {e+1}/{cfg.finetune_epochs}\", leave=False)\n        for x, y, _ in pbar:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.amp.autocast('cuda'):\n                lo = model(x)\n                ls = criterion(lo, y)\n            scaler.scale(ls).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            \n            bs = x.size(0)\n            loss_sum += ls.item() * bs\n            acc_sum += (lo.argmax(1) == y).float().sum().item()\n            n += bs\n            pbar.set_postfix(loss=f\"{loss_sum/n:.4f}\", acc=f\"{acc_sum/n:.4f}\")\n            \n        tr_loss, tr_acc = loss_sum / n, acc_sum / n\n        va_loss, va_acc, va_f1, _ = validate(return_details=True, epoch_idx=cfg.epochs + e + 1)\n        \n        print(f\"[FT {e+1}/{cfg.finetune_epochs}] train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n              f\"val_loss={va_loss:.4f} acc={va_acc:.4f} f1={va_f1:.4f} | time={fmt_time(time.time()-t0)}\")\n        \n        if va_acc > best_acc:\n            best_acc = va_acc\n            torch.save({\"model\": model.state_dict(), \"classes\": classes}, best_path)\n            print(f\"  -> Saved new best after FT (val_acc={best_acc:.4f})\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T15:41:39.226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11. Inference & Submission\n#### This block loads the final best model, runs inference on the test set, and generates a submission.csv file in the required format for the Kaggle competition. ","metadata":{}},{"cell_type":"code","source":"print(\"\\n[Inference] Preparing for test set prediction...\")\n# Load the final best model\nckpt = torch.load(best_path, map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"])\nmodel.eval().to(device)\n\n# Use the appropriate image size for evaluation\neval_size = 288 if cfg.do_finetune_288 else cfg.img_size\neval_tfm = T.Compose([\n    T.Resize(int(eval_size * 1.14)),\n    T.CenterCrop(eval_size),\n    T.ToTensor(),\n    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\n@torch.no_grad()\ndef tta_predict(pil, tfm):\n    \"\"\"Performs Test-Time Augmentation (TTA) prediction.\"\"\"\n    xs = [tfm(pil)]\n    if cfg.do_tta:\n        xs.append(T.functional.hflip(xs[0].clone())) # Add horizontally flipped version\n    x = torch.stack(xs).to(device)\n    with torch.amp.autocast('cuda'):\n        # Average predictions from original and augmented images\n        pr = model(x).softmax(1).mean(0).cpu().numpy()\n    return pr\n\nTEST_DIR = TEST if TEST.exists() else None\nif TEST_DIR is not None:\n    test_paths = sorted([p for p in TEST_DIR.iterdir() if p.suffix.lower() in [\".jpg\", \".png\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"]])\n    rows = []\n    for p in tqdm(test_paths, desc=\"Infer test\"):\n        pil = Image.open(p).convert(\"RGB\")\n        pred = int(np.argmax(tta_predict(pil, eval_tfm)))\n        rows.append({\"file\": p.name, \"species\": classes[pred]})\n    \n    sub = pd.DataFrame(rows)\n    out_csv = Path(cfg.save_dir) / f\"submission_{cfg.model_name}_{'pt' if cfg.use_pretrained else 'scratch'}.csv\"\n    sub.to_csv(out_csv, index=False)\n    print(f\">> Saved submission to: {out_csv}\")\n    display(sub.head())\nelse:\n    print(\"Test directory not found, skipping submission generation.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T15:41:39.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 12. Feature Dump for Visualization\n#### To understand what the model has learned, we can extract the feature vectors from the validation set. These features can then be visualized using dimensionality reduction techniques like t-SNE or UMAP to see if the classes form distinct clusters.","metadata":{}},{"cell_type":"code","source":"def get_feature_extractor(model):\n    \"\"\"Returns a hook to extract features from the model's penultimate layer.\"\"\"\n    name = cfg.model_name.lower()\n    model.eval()\n    if name == \"resnet18\":\n        backbone = nn.Sequential(*(list(model.children())[:-1])) # Exclude the final fc layer\n        def fe(x): return backbone(x).view(x.size(0), -1)\n        return fe\n    elif name == \"convnext_tiny\":\n        feats = nn.Sequential(model.features, model.avgpool)\n        def fe(x): return feats(x).view(x.size(0), -1)\n        return fe\n    elif name == \"customcnn\":\n        stem = nn.Sequential(model.stem, model.stage1, model.down1, model.stage2, model.down2, model.stage3)\n        gap = nn.AdaptiveAvgPool2d(1)\n        def fe(x): return gap(stem(x)).view(x.size(0), -1)\n        return fe\n    else:\n        raise ValueError(\"Feature extractor not defined for this model.\")\n\nif cfg.enable_feature_dump:\n    print(\"\\n[Features] Dumping validation set features for t-SNE/UMAP...\")\n    fe = get_feature_extractor(model)\n    feats, ys, files = [], [], []\n    for x, y, paths in tqdm(valid_loader, desc=\"Extracting val features\"):\n        with torch.no_grad():\n            f = fe(x.to(device)).cpu().numpy()\n        feats.append(f)\n        ys.append(y.numpy())\n        files.extend(list(paths))\n    \n    feats = np.concatenate(feats, 0)\n    ys = np.concatenate(ys, 0)\n    \n    # Save features, labels, and file index\n    np.save(os.path.join(cfg.log_dir, \"val_feats.npy\"), feats)\n    np.save(os.path.join(cfg.log_dir, \"val_labels.npy\"), ys)\n    pd.DataFrame({\"path\": files, \"label\": [classes[i] for i in ys]}).to_csv(\n        os.path.join(cfg.log_dir, \"val_index.csv\"), index=False\n    )\n    print(f\"[Features] Saved! Features shape: {feats.shape}, Labels shape: {ys.shape}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T15:41:39.227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 13. Grad-CAM Visualization\n#### This final block generates Grad-CAM (Gradient-weighted Class Activation Mapping) visualizations. These heatmaps highlight the regions of the image that were most important for the model's prediction, helping us interpret its decisions.","metadata":{}},{"cell_type":"code","source":"def get_cam_target_layer(model):\n    \"\"\"Identifies the target layer for Grad-CAM for different architectures.\"\"\"\n    name = cfg.model_name.lower()\n    if name == \"resnet18\":\n        return model.layer4[-1].conv2\n    elif name == \"convnext_tiny\":\n        # Target the last depthwise conv in the final stage\n        try:\n            return model.features[-1][-1].block[0]\n        except Exception: # Fallback for different torchvision versions\n            for m in model.features.modules():\n                if isinstance(m, nn.Conv2d) and m.groups == m.in_channels:\n                    layer = m\n            if layer is None: raise RuntimeError(\"No depthwise conv found for ConvNeXt CAM.\")\n            return layer\n    elif name == \"customcnn\":\n        return model.stage3[-1].dw\n    else:\n        return None\n\ndef build_cam_handle(model):\n    \"\"\"Sets up forward and backward hooks to capture gradients and activations.\"\"\"\n    target_layer = get_cam_target_layer(model)\n    if target_layer is None:\n        print(f\"[CAM] Not supported for model {cfg.model_name}, skipping.\")\n        return None, None\n    \n    activations = {}\n    def fwd_hook(m, inp, out): activations[\"feat\"] = out.detach()\n    def bwd_hook(m, gin, gout): activations[\"grad\"] = gout[0].detach()\n    h1 = target_layer.register_forward_hook(fwd_hook)\n    h2 = target_layer.register_backward_hook(bwd_hook)\n    return activations, (h1, h2)\n\ndef save_cam_image(pil_img, cam_01, path_out, alpha=0.4):\n    \"\"\"Overlays the CAM heatmap on the original image and saves it.\"\"\"\n    try:\n        import cv2 # Use OpenCV for colormapping if available\n        rgb = np.array(pil_img)\n        heat = (cam_01 * 255).astype(np.uint8)\n        heat = cv2.applyColorMap(heat, cv2.COLORMAP_JET)[:, :, ::-1] # BGR to RGB\n        over = (alpha * heat + (1 - alpha) * rgb).clip(0, 255).astype(np.uint8)\n        Image.fromarray(over).save(path_out)\n    except ImportError: # Fallback if cv2 is not installed\n        rgb = np.array(pil_img).astype(np.float32)\n        heat = np.zeros_like(rgb); heat[..., 0] = cam_01 * 255.0 # Simple red heatmap\n        over = (alpha * heat + (1 - alpha) * rgb).clip(0, 255).astype(np.uint8)\n        Image.fromarray(over).save(path_out)\n\nif cfg.enable_cam:\n    cam_dir = os.path.join(cfg.log_dir, \"cam\")\n    os.makedirs(cam_dir, exist_ok=True)\n    act, hooks = build_cam_handle(model)\n    \n    if act is not None:\n        print(\"\\n[CAM] Exporting Grad-CAM samples...\")\n        model.eval()\n        cnt = 0\n        # Iterate through validation data to find samples\n        for x, y, paths in valid_loader:\n            for i in range(x.size(0)):\n                if cnt >= cfg.cam_samples: break\n                xi, yi = x[i:i + 1].to(device), y[i:i + 1].to(device)\n                pil = Image.open(paths[i]).convert(\"RGB\").resize((eval_size, eval_size))\n                \n                # Get model output for the target class\n                model.zero_grad(set_to_none=True)\n                with torch.amp.autocast('cuda'):\n                    lo = model(xi)\n                    score = lo[0, yi.item()]\n                score.backward() # Backpropagate to get gradients\n                \n                feat, grad = act.get(\"feat\"), act.get(\"grad\")\n                if feat is None or grad is None: continue\n\n                # Grad-CAM calculation\n                weights = grad.mean(dim=(2, 3), keepdim=True)\n                cam = torch.relu((feat * weights).sum(1, keepdim=True))\n                cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6) # Normalize to [0,1]\n                cam = T.functional.resize(cam, [eval_size, eval_size])[0, 0].cpu().numpy()\n\n                outp = os.path.join(cam_dir, f\"{cnt:02d}_{classes[yi.item()]}.jpg\")\n                save_cam_image(pil, cam, outp)\n                cnt += 1\n            if cnt >= cfg.cam_samples: break\n        \n        for h in hooks: h.remove() # Clean up hooks\n        print(f\"[CAM] Saved {cnt} images to -> {cam_dir}\")\n\nprint(\"\\nAll done\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-29T15:41:39.227Z"}},"outputs":[],"execution_count":null}]}