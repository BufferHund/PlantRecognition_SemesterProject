{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7880,"databundleVersionId":862031,"sourceType":"competition"},{"sourceId":13171585,"sourceType":"datasetVersion","datasetId":8346609}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Experiment 4 – Large-Scale Noisy Pretraining Impact\n\n#### The final experiment evaluates the effect of large-scale but noisier pretraining using the iNaturalist dataset. By transferring this pretrained model to the Plant Seedlings dataset with two strategies—full fine-tuning and linear probing—it assesses how large, diverse datasets can compete with smaller but in-domain pretraining when the entire model is fine-tuned","metadata":{}},{"cell_type":"markdown","source":"### 1. Experiment Overview & Configuration\n#### This block sets up the main configuration for the transfer learning experiment. You can adjust all the hyperparameters here, such as the training strategy, model architecture, paths, and training settings.","metadata":{}},{"cell_type":"code","source":"\nCFG = {\n    \"seed\": 42,\n    \"strategy\": \"full_ft\",        # \"full_ft\" | \"lp_unfreeze\"\n    \"model_name\": \"convnext_tiny\",# \"convnext_tiny\" | \"resnet18\"\n    # <<< Change the path below to your best pre-trained weights (*.pt) from the large dataset >>>\n    \"ckpt_path\": \"/kaggle/input/inatweight/convnext_tiny_inat_pretrain_best.pt\",\n\n    # Data Path (Kaggle's official Plant Seedlings dataset)\n    \"data_root\": \"/kaggle/input/plant-seedlings-classification\",\n    \"val_ratio\": 0.1,\n\n    # Training Configuration\n    \"img_size\": 224,\n    \"batch_size\": 128,\n    \"num_workers\": 2,\n    \"epochs_fullft\": 30,          # Number of epochs for full fine-tuning\n    \"epochs_lp\": 5,               # Number of epochs for linear probing (backbone frozen)\n    \"epochs_unfreeze\": 15,        # Number of epochs for gradual unfreezing\n    \"lr_head\": 1e-3,              # Learning rate for the classification head\n    \"lr_backbone\": 3e-4,          # Learning rate for the backbone (used in full_ft or unfreeze phase)\n    \"weight_decay\": 1e-2,\n    \"label_smoothing\": 0.1,\n    \"mixed_precision\": True,\n\n    \"out_root\": \"/kaggle/working/seedlings_transfer\",\n    \"run_name\": None,             # If None, automatically named based on strategy/model\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:19.823705Z","iopub.execute_input":"2025-09-29T16:17:19.824537Z","iopub.status.idle":"2025-09-29T16:17:19.829301Z","shell.execute_reply.started":"2025-09-29T16:17:19.824500Z","shell.execute_reply":"2025-09-29T16:17:19.828563Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### 2. Imports & Environment Setup\n#### This block handles all necessary imports and sets up the basic environment, including device selection (GPU/CPU) and a function to ensure reproducibility by setting random seeds.","metadata":{}},{"cell_type":"code","source":"import os, math, random, time, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport torchvision\nfrom torchvision import transforms as T, models\n\n# ---------------------------\n# Basic Utilities\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    \"\"\"Sets random seeds for reproducibility.\"\"\"\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:19.830499Z","iopub.execute_input":"2025-09-29T16:17:19.830780Z","iopub.status.idle":"2025-09-29T16:17:19.850484Z","shell.execute_reply.started":"2025-09-29T16:17:19.830737Z","shell.execute_reply":"2025-09-29T16:17:19.849787Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### 3. Data Preparation\n#### This section contains all the functions needed to prepare the dataset. It includes splitting the data into stratified training and validation sets, defining a custom Dataset class, and creating data augmentation pipelines (transforms) for training and validation.","metadata":{}},{"cell_type":"code","source":"def ensure_dir(p: Path):\n    \"\"\"Create directory if it does not exist.\"\"\"\n    p.mkdir(parents=True, exist_ok=True)\n\ndef build_seedlings_split(root: Path, val_ratio=0.1, seed=42):\n    \"\"\"\n    Creates a stratified train/validation split from the Plant Seedlings dataset directory.\n    \"\"\"\n    train_dir = root/\"train\"\n    assert train_dir.exists(), f\"Plant Seedlings data not found: {train_dir}\"\n    classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n    c2i = {c:i for i,c in enumerate(classes)}\n    items=[]\n    exts={\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n    for c in classes:\n        for p in (train_dir/c).glob(\"*.*\"):\n            if p.suffix.lower() in exts:\n                items.append((p, c2i[c]))\n    y = np.array([b for _,b in items]); idx = np.arange(len(items))\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n    tr, va = next(sss.split(idx,y))\n    return classes, [items[i] for i in tr], [items[i] for i in va]\n\nclass DS(Dataset):\n    \"\"\"Custom Dataset for loading images.\"\"\"\n    def __init__(self, items, tfm): self.items, self.tfm = items, tfm\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        p, y = self.items[i]\n        img = Image.open(p).convert(\"RGB\")\n        x = self.tfm(img)\n        return x, y, str(p)\n\ndef get_tfms(size):\n    \"\"\"\n    Returns training and validation transforms.\n    \"\"\"\n    train = T.Compose([\n        T.Resize(int(size*1.14)),\n        T.RandomResizedCrop(size, scale=(0.85,1.0), ratio=(3/4,4/3)),\n        T.RandomHorizontalFlip(0.5),\n        T.RandAugment(2, 9),\n        T.ToTensor(),\n        T.RandomErasing(p=0.1),\n        T.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n    ])\n    valid = T.Compose([\n        T.Resize(int(size*1.14)),\n        T.CenterCrop(size),\n        T.ToTensor(),\n        T.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n    ])\n    return train, valid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:19.851508Z","iopub.execute_input":"2025-09-29T16:17:19.851777Z","iopub.status.idle":"2025-09-29T16:17:19.873218Z","shell.execute_reply.started":"2025-09-29T16:17:19.851761Z","shell.execute_reply":"2025-09-29T16:17:19.872609Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### 4. Evaluation and Logging Utilities\n#### These functions are used for model evaluation and for saving the results. They calculate metrics, generate and save confusion matrices and classification reports, and log the training progress to a CSV file.","metadata":{}},{"cell_type":"code","source":"def accuracy(out, tgt):\n    \"\"\"Calculates accuracy.\"\"\"\n    with torch.no_grad():\n        return (out.argmax(1)==tgt).float().mean().item()\n\n@torch.no_grad()\ndef evaluate(model, loader, num_classes):\n    \"\"\"\n    Evaluates the model on a given dataloader.\n    Returns loss, accuracy, F1-score, confusion matrix, and predictions.\n    \"\"\"\n    model.eval()\n    all_pred=[]; all_true=[]; loss_sum=acc_sum=n=0\n    crit = nn.CrossEntropyLoss()\n    for x,y,_ in loader:\n        x=x.to(device); y=y.to(device)\n        with torch.amp.autocast(\"cuda\", enabled=CFG[\"mixed_precision\"]):\n            lo = model(x); ls = crit(lo,y)\n        bs=x.size(0)\n        loss_sum += ls.item()*bs\n        acc_sum  += accuracy(lo,y)*bs\n        n += bs\n        all_pred.append(lo.argmax(1).cpu().numpy())\n        all_true.append(y.cpu().numpy())\n    pred = np.concatenate(all_pred); true = np.concatenate(all_true)\n    f1 = f1_score(true, pred, average=\"macro\")\n    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n    return loss_sum/n, acc_sum/n, f1, cm, true, pred\n\ndef save_cm_and_report(cm, y_true, y_pred, class_names, out_dir: Path, epoch: int):\n    \"\"\"\n    Saves the confusion matrix and a per-class classification report to CSV files.\n    \"\"\"\n    ensure_dir(out_dir)\n    df_cm = pd.DataFrame(cm, columns=class_names)\n    df_cm.insert(0, \"true\\\\pred\", class_names)\n    df_cm.to_csv(out_dir / f\"confusion_matrix_epoch{epoch:03d}.csv\", index=False)\n\n    rep = classification_report(y_true, y_pred, labels=list(range(len(class_names))),\n                                 target_names=class_names, output_dict=True, zero_division=0)\n    pd.DataFrame(rep).T.reset_index().rename(columns={\"index\":\"class\"}).to_csv(\n        out_dir / f\"per_class_report_epoch{epoch:03d}.csv\", index=False\n    )\n\ndef save_curve(curves, out_csv: Path):\n    \"\"\"Saves the training history (loss, accuracy, F1) to a CSV file.\"\"\"\n    pd.DataFrame(curves).to_csv(out_csv, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:19.873865Z","iopub.execute_input":"2025-09-29T16:17:19.874089Z","iopub.status.idle":"2025-09-29T16:17:19.894330Z","shell.execute_reply.started":"2025-09-29T16:17:19.874062Z","shell.execute_reply":"2025-09-29T16:17:19.893617Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### 5. Model Building & Checkpoint Loading\n#### This section defines functions to construct the model and to load the backbone weights from a pre-trained checkpoint. The load_backbone_from_ckpt function is designed to discard the old classification head and only load the feature extractor weights.","metadata":{}},{"cell_type":"code","source":"def build_model(num_classes, name=\"convnext_tiny\"):\n    \"\"\"\n    Builds the specified model with a new classification head.\n    \"\"\"\n    n = name.lower()\n    if n == \"convnext_tiny\":\n        m = models.convnext_tiny(weights=None)\n        m.classifier[2] = nn.Linear(m.classifier[2].in_features, num_classes)\n        return m\n    elif n == \"resnet18\":\n        m = models.resnet18(weights=None)\n        m.fc = nn.Linear(m.fc.in_features, num_classes)\n        return m\n    else:\n        raise ValueError(f\"Unsupported model_name: {name}\")\n\ndef load_backbone_from_ckpt(model, ckpt_path, model_name):\n    \"\"\"\n    Loads only the backbone weights, discarding the old classification head.\n    The checkpoint can be a state_dict or a dictionary containing a 'state_dict' key.\n    \"\"\"\n    assert Path(ckpt_path).exists(), f\"Checkpoint not found: {ckpt_path}\"\n    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n    sd = ckpt.get(\"state_dict\", ckpt)  # Compatible with both state_dict formats\n\n    new_sd = model.state_dict()\n    drop_keys = []\n    if model_name == \"convnext_tiny\":\n        drop_prefixes = [\"classifier.\"]\n    else:  # resnet18\n        drop_prefixes = [\"fc.\"]\n\n    # Remove the old classification head from the checkpoint state_dict\n    for k in list(sd.keys()):\n        if any(k.startswith(p) for p in drop_prefixes):\n            drop_keys.append(k)\n            sd.pop(k)\n\n    # Load only keys that match in name and shape\n    matched = {k: v for k, v in sd.items() if (k in new_sd and new_sd[k].shape == v.shape)}\n    missing = [k for k in new_sd.keys() if k not in matched]\n    print(f\"[ckpt] load backbone: matched={len(matched)} | missing(new head etc.)={len(missing)} | dropped_old_head={len(drop_keys)}\")\n\n    new_state = model.state_dict()\n    new_state.update(matched)\n    model.load_state_dict(new_state)\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:20.037478Z","iopub.execute_input":"2025-09-29T16:17:20.038123Z","iopub.status.idle":"2025-09-29T16:17:20.045320Z","shell.execute_reply.started":"2025-09-29T16:17:20.038096Z","shell.execute_reply":"2025-09-29T16:17:20.044494Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### 6. Freezing/Unfreezing Utilities\n#### These helper functions control which parts of the model are trainable. This is crucial for implementing the linear probing and gradual unfreezing strategies. They correctly handle setting requires_grad for parameters and managing the training/evaluation mode of normalization layers (BatchNorm/LayerNorm).","metadata":{}},{"cell_type":"code","source":"def set_backbone_trainable(model, model_name, trainable: bool):\n    \"\"\"\n    Sets the entire backbone to be trainable or frozen.\n    Also handles the train/eval mode of normalization layers.\n    \"\"\"\n    if model_name==\"convnext_tiny\":\n        backbone = [model.features]  # ConvNeXt's backbone is the 'features' module\n    else:\n        backbone = [nn.Sequential(model.conv1, model.bn1, model.layer1, model.layer2, model.layer3, model.layer4)]\n    \n    for m in backbone:\n        for p in m.parameters(): p.requires_grad = trainable\n        # For BN/LN: set to eval() when frozen, and train() when trainable\n        for mm in m.modules():\n            if isinstance(mm, (nn.BatchNorm2d, nn.LayerNorm)):\n                mm.eval() if not trainable else mm.train()\n\ndef unfreeze_last_stages(model, model_name, stages=1):\n    \"\"\"\n    Unfreezes the last N stages of the backbone. Useful for the gradual unfreezing strategy.\n    convnext_tiny: The 'features' module contains sequential blocks.\n    resnet18: Has 4 main layers (layer1 to layer4).\n    \"\"\"\n    set_backbone_trainable(model, model_name, trainable=False)  # Freeze everything first\n    if model_name==\"convnext_tiny\":\n        blocks = model.features  # Sequential module\n        to_unfreeze = list(range(len(blocks)-stages, len(blocks)))\n        for i in to_unfreeze:\n            for p in blocks[i].parameters(): p.requires_grad=True\n            for mm in blocks[i].modules():\n                if isinstance(mm, (nn.BatchNorm2d, nn.LayerNorm)): mm.train()\n    else: # resnet18\n        layers = [model.layer1, model.layer2, model.layer3, model.layer4]\n        for l in layers[-stages:]:\n            for p in l.parameters(): p.requires_grad=True\n            for mm in l.modules():\n                if isinstance(mm, nn.BatchNorm2d): mm.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:20.046527Z","iopub.execute_input":"2025-09-29T16:17:20.046875Z","iopub.status.idle":"2025-09-29T16:17:20.066346Z","shell.execute_reply.started":"2025-09-29T16:17:20.046851Z","shell.execute_reply":"2025-09-29T16:17:20.065496Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### 7. Optimizer and Training Loop\n#### This block defines the make_optimizer function, which can create an optimizer with different learning rates for the backbone and the head (discriminative learning rates). The train_one_phase function contains the complete training and validation loop for a given number of epochs.","metadata":{}},{"cell_type":"code","source":"def make_optimizer(model, model_name, lr_backbone, lr_head, wd=1e-2, full=False):\n    \"\"\"\n    Creates an AdamW optimizer. Can set a smaller learning rate for the backbone.\n    \"\"\"\n    if model_name==\"convnext_tiny\":\n        head_params = list(model.classifier.parameters())\n        bb_params   = list(model.features.parameters())\n    else: # resnet18\n        head_params = list(model.fc.parameters())\n        bb_params   = [p for n,p in model.named_parameters() if not n.startswith(\"fc.\")]\n    \n    params=[]\n    if full:\n        # Discriminative LR: use a smaller LR for the backbone\n        params=[{\"params\": bb_params, \"lr\": lr_backbone},\n                {\"params\": head_params, \"lr\": lr_head}]\n    else: # Only train the head\n        params=[{\"params\": head_params, \"lr\": lr_head}]\n        \n    return torch.optim.AdamW(params, weight_decay=wd)\n\ndef train_one_phase(model, train_loader, valid_loader, epochs, opt, crit, curves, out_met_dir, class_names, start_epoch=1):\n    \"\"\"\n    Runs the main training and validation loop for a specified number of epochs.\n    \"\"\"\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=CFG[\"mixed_precision\"])\n    best_f1=-1; best_ep=-1; best_path = out_met_dir.parent / f\"{run_name}_best.pt\"\n    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1,epochs))\n    \n    for e in range(start_epoch, start_epoch+epochs):\n        model.train(); t0=time.time()\n        loss_sum=acc_sum=n=0\n        for x,y,_ in train_loader:\n            x=x.to(device); y=y.to(device)\n            opt.zero_grad(set_to_none=True)\n            with torch.amp.autocast(\"cuda\", enabled=CFG[\"mixed_precision\"]):\n                lo = model(x)\n                ls = crit(lo,y)\n            scaler.scale(ls).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            \n            loss_sum += ls.item()*x.size(0)\n            acc_sum  += accuracy(lo,y)*x.size(0)\n            n += x.size(0)\n        sched.step()\n\n        tr_loss, tr_acc = loss_sum/n, acc_sum/n\n        va_loss, va_acc, va_f1, cm, y_true, y_pred = evaluate(model, valid_loader, num_classes=len(class_names))\n        print(f\"[{run_name}] Epoch {e:02d}/{start_epoch+epochs-1:02d} | \"\n              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f} f1={va_f1:.4f} | time={int(time.time()-t0)}s\")\n\n        curves[\"epoch\"].append(e)\n        curves[\"train_loss\"].append(tr_loss)\n        curves[\"train_acc\"].append(tr_acc)\n        curves[\"val_acc\"].append(va_acc)\n        curves[\"val_f1\"].append(va_f1)\n        save_curve(curves, out_met_dir/\"train_curve.csv\")\n        save_cm_and_report(cm, y_true, y_pred, class_names, out_met_dir, e)\n\n        if va_f1 > best_f1:\n            best_f1, best_ep = va_f1, e\n            torch.save({\"state_dict\": model.state_dict(), \"classes\": class_names}, best_path)\n            \n    return best_f1, best_ep, best_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:20.067012Z","iopub.execute_input":"2025-09-29T16:17:20.067259Z","iopub.status.idle":"2025-09-29T16:17:20.087023Z","shell.execute_reply.started":"2025-09-29T16:17:20.067243Z","shell.execute_reply":"2025-09-29T16:17:20.086355Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### 8. Main Execution Flow","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# Main Execution\n# ---------------------------\nset_seed(CFG[\"seed\"])\n\n# --- Data Loading ---\nroot = Path(CFG[\"data_root\"])\nclasses, tr_items, va_items = build_seedlings_split(root, CFG[\"val_ratio\"], CFG[\"seed\"])\nnum_classes = len(classes)\ntrain_tfm, valid_tfm = get_tfms(CFG[\"img_size\"])\ntrain_ds, valid_ds = DS(tr_items, train_tfm), DS(va_items, valid_tfm)\ntrain_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True)\n\n# --- Naming and Output Directory ---\nif CFG[\"run_name\"] is None:\n    run_name = f\"seedlings_{CFG['model_name']}_{CFG['strategy']}\"\nelse:\n    run_name = CFG[\"run_name\"]\nout_dir = Path(CFG[\"out_root\"])/run_name\nmet_dir = out_dir/\"metrics\"\nensure_dir(met_dir)\n\n# --- Model Creation and Weight Loading ---\n# Build a new model with a head for the Seedlings dataset, then load the pre-trained backbone.\nmodel = build_model(num_classes, CFG[\"model_name\"]).to(device)\nmodel = load_backbone_from_ckpt(model, CFG[\"ckpt_path\"], CFG[\"model_name\"])\n\n# --- Log Resource Info ---\nwith open(met_dir/\"resource.json\", \"w\") as f:\n    json.dump({\n        \"strategy\": CFG[\"strategy\"],\n        \"model\": CFG[\"model_name\"],\n        \"img_size\": CFG[\"img_size\"],\n        \"batch_size\": CFG[\"batch_size\"],\n        \"params\": sum(p.numel() for p in model.parameters())\n    }, f)\n\n# --- Training Execution based on Strategy ---\ncurves = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_f1\": []}\ncrit = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smoothing\"])\nbest_f1 = -1\nbest_ep = -1\n\nif CFG[\"strategy\"] == \"full_ft\":\n    # Full fine-tuning: make backbone trainable; use discriminative LR\n    print(\"--- Starting Full Fine-Tuning ---\")\n    set_backbone_trainable(model, CFG[\"model_name\"], trainable=True)\n    opt = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                         wd=CFG[\"weight_decay\"], full=True)\n    best_f1, best_ep, best_path = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_fullft\"], opt, crit, curves, met_dir, classes\n    )\n\nelif CFG[\"strategy\"] == \"lp_unfreeze\":\n    # STEP 1: Linear Probing - freeze backbone, train only the head\n    print(\"--- Starting Phase 1: Linear Probing ---\")\n    set_backbone_trainable(model, CFG[\"model_name\"], trainable=False)\n    opt = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                         wd=CFG[\"weight_decay\"], full=False)\n    best_f1, best_ep, best_path = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_lp\"], opt, crit, curves, met_dir, classes, start_epoch=1\n    )\n\n    # STEP 2: Gradual Unfreezing - unfreeze the last N stages and train with discriminative LR\n    print(\"\\n--- Starting Phase 2: Gradual Unfreezing ---\")\n    unfreeze_last_stages(model, CFG[\"model_name\"], stages=1 if CFG[\"model_name\"]==\"convnext_tiny\" else 2)\n    opt = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                         wd=CFG[\"weight_decay\"], full=True)\n    b2_f1, b2_ep, best_path_2 = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_unfreeze\"], opt, crit, curves, met_dir, classes,\n        start_epoch=(CFG[\"epochs_lp\"]+1)\n    )\n    if b2_f1 > best_f1:\n        best_f1, best_ep = b2_f1, b2_ep\n\nelse:\n    raise ValueError(\"CFG['strategy'] must be 'full_ft' or 'lp_unfreeze'\")\n\nprint(f\"\\nDone. Best macro-F1={best_f1:.4f} @ epoch {best_ep}.\")\nprint(\"Outputs ->\", out_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:17:20.088289Z","iopub.execute_input":"2025-09-29T16:17:20.088528Z","execution_failed":"2025-09-29T16:18:46.281Z"}},"outputs":[{"name":"stdout","text":"[ckpt] load backbone: matched=178 | missing(new head etc.)=4 | dropped_old_head=4\n--- Starting Full Fine-Tuning ---\n[seedlings_convnext_tiny_full_ft] Epoch 01/30 | train_loss=1.2627 acc=0.7180 | val_loss=0.3377 acc=0.9200 f1=0.8956 | time=79s\n","output_type":"stream"}],"execution_count":null}]}