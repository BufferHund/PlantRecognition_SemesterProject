{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1003081,"sourceType":"datasetVersion","datasetId":550485}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Experiment 2 – Large-Scale Dataset Generalization\n\n#### In this stage, the study tests ResNet-18 and ConvNeXt-Tiny on the larger, more complex LeafSnap dataset. This experiment explores how model capacity and architecture impact performance under fine-grained, high-diversity conditions, and examines the generalization power of pretrained models beyond controlled environments","metadata":{}},{"cell_type":"markdown","source":"### 1. Imports\n#### This cell imports all the necessary libraries for the project, including PyTorch, Torchvision, NumPy, Pandas, and Scikit-learn.","metadata":{}},{"cell_type":"code","source":"import os, json, math, random, time, re, glob, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n\nimport torchvision\nfrom torchvision import transforms, datasets\nfrom torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights, resnet18, ResNet18_Weights\n\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:30.921410Z","iopub.execute_input":"2025-09-29T16:47:30.921694Z","iopub.status.idle":"2025-09-29T16:47:40.716786Z","shell.execute_reply.started":"2025-09-29T16:47:30.921674Z","shell.execute_reply":"2025-09-29T16:47:40.716106Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 2. Configuration\n#### This cell contains all the hyperparameters and settings for the experiment in a single dictionary CFG. \n\n","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Configuration\n# -----------------------------\nCFG = {\n    \"seed\": 42,\n    \"data_root_candidates\": [\n        \"/kaggle/input/leafsnap-dataset/leafsnap-dataset/dataset/images\",\n        \"/kaggle/input/leafsnap-dataset/leafsnap-dataset/dataset\",      # Fallback path\n    ],\n    \"use_subdirs\": [\"field\", \"lab\"],     # Use both domains; change to [\"field\"] to use only field images\n    \"min_per_class\": 80,                 # Minimum images per class to be included\n    \"max_classes\": 50,                   # Maximum number of classes to use (Top-K by sample count)\n    \"split_ratio\": 0.85,                 # Training set proportion (stratified within each class)\n    \"balance_train\": True,               # Whether to use class balancing for the training set\n    \"img_size\": 224,\n    \"batch_size\": 64,\n    \"num_workers\": 4,\n    \"model_name\": \"convnext_tiny\",       # Options: convnext_tiny | resnet18 | customcnn\n    \"pretrained\": True,\n    \"epochs\": 20,\n    \"lr\": 2e-4,\n    \"weight_decay\": 1e-4,\n    \"mixed_precision\": True,\n    \"enable_cam\": True,\n    \"cam_max_images\": 12,\n    \"out_root\": \"/kaggle/working/leafsnap_runs\",\n    \"run_name\": \"convnext_tiny_leafsnap_phase4\",\n    \"save_every\": 1,                     # Save confusion matrix/report every N epochs\n}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.718047Z","iopub.execute_input":"2025-09-29T16:47:40.718471Z","iopub.status.idle":"2025-09-29T16:47:40.779983Z","shell.execute_reply.started":"2025-09-29T16:47:40.718451Z","shell.execute_reply":"2025-09-29T16:47:40.779011Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 3. Utility Functions & Custom Dataset\n#### This cell defines helper functions for reproducibility (set_seed), directory management (ensure_dir), and automatically finding the data path (auto_find_data_root).","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Utility Functions\n# -----------------------------\ndef set_seed(seed=42):\n    \"\"\"Sets the seed for reproducibility.\"\"\"\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef ensure_dir(p: Path):\n    \"\"\"Creates a directory if it does not exist.\"\"\"\n    p.mkdir(parents=True, exist_ok=True)\n\ndef auto_find_data_root():\n    \"\"\"Automatically finds the dataset root directory from a list of candidates.\"\"\"\n    for p in CFG[\"data_root_candidates\"]:\n        if Path(p).exists():\n            return Path(p)\n    raise FileNotFoundError(\n        f\"No valid dataset root found. Checked: {CFG['data_root_candidates']}.\\n\"\n        \"Mount the dataset 'leafsnap-dataset' in your notebook first (Add data -> Kaggle dataset).\"\n    )\n\nclass ListDataset(Dataset):\n    \"\"\"A custom dataset that takes a list of (path, label) samples.\"\"\"\n    def __init__(self, samples, transform=None):\n        self.samples = samples\n        self.transform = transform\n    def __len__(self): return len(self.samples)\n    def __getitem__(self, i):\n        path, y = self.samples[i]\n        # Read image, handle RGBA, and convert to PIL for transforms\n        img = torchvision.io.read_image(path).float()/255.0\n        if img.size(0) == 4: img = img[:3, ...] # Handle RGBA by dropping alpha\n        img = transforms.ToPILImage()(img)\n        if self.transform: img = self.transform(img)\n        return img, y, path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.781077Z","iopub.execute_input":"2025-09-29T16:47:40.781600Z","iopub.status.idle":"2025-09-29T16:47:40.817846Z","shell.execute_reply.started":"2025-09-29T16:47:40.781482Z","shell.execute_reply":"2025-09-29T16:47:40.816961Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### 4. Data Loading Pipeline\n#### This section contains the core logic for preparing the data. It scans the data directories, filters classes based on sample count, performs a stratified train/validation split, defines image augmentations, and finally creates the DataLoader instances for training and validation.","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Data Loading Pipeline\n# -----------------------------\ndef build_index(root: Path, use_subdirs):\n    \"\"\"\n    Scans directories, merges classes from {field, lab}, filters by sample count,\n    and returns a dictionary mapping class names to their file lists.\n    \"\"\"\n    class_to_files = {}\n    for sub in use_subdirs:\n        d = root / sub\n        if not d.exists(): continue\n        for cls_dir in sorted(d.glob(\"*\")):\n            if not cls_dir.is_dir(): continue\n            cls_name = cls_dir.name\n            files = sorted(list(cls_dir.glob(\"*.jpg\")) + list(cls_dir.glob(\"*.png\")))\n            if not files: continue\n            class_to_files.setdefault(cls_name, []).extend(files)\n    \n    # Filter by min samples and sort to get top-k classes\n    items = [(k, v) for k, v in class_to_files.items() if len(v) >= CFG[\"min_per_class\"]]\n    if not items:\n        raise RuntimeError(\"No class meets min_per_class. Try lowering CFG['min_per_class'].\")\n    items.sort(key=lambda kv: len(kv[1]), reverse=True)\n    if CFG[\"max_classes\"] is not None:\n        items = items[:CFG[\"max_classes\"]]\n    return {k: v for k, v in items}\n\ndef stratified_split_by_class(files_by_class, split_ratio):\n    \"\"\"\n    Performs a stratified train/val split within each class.\n    Returns [(path, class_idx), ...], [(path, class_idx), ...], [class_names]\n    \"\"\"\n    classes = sorted(files_by_class.keys())\n    cls_to_idx = {c: i for i, c in enumerate(classes)}\n    train_list, val_list = [], []\n    for c in classes:\n        files = files_by_class[c]\n        random.shuffle(files)\n        n = len(files); n_train = int(n * split_ratio)\n        train_list.extend([(str(p), cls_to_idx[c]) for p in files[:n_train]])\n        val_list.extend([(str(p), cls_to_idx[c]) for p in files[n_train:]])\n    return train_list, val_list, classes\n\ndef build_dataloaders():\n    \"\"\"Builds and returns train/validation dataloaders and class names.\"\"\"\n    data_root = auto_find_data_root()\n    files_by_class = build_index(data_root, CFG[\"use_subdirs\"])\n    train_list, val_list, class_names = stratified_split_by_class(files_by_class, CFG[\"split_ratio\"])\n\n    # Image transformations\n    mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n    train_tf = transforms.Compose([\n        transforms.RandomResizedCrop(CFG[\"img_size\"], scale=(0.6, 1.0)),\n        transforms.RandAugment(num_ops=2, magnitude=7),\n        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n    val_tf = transforms.Compose([\n        transforms.Resize(CFG[\"img_size\"] + 32),\n        transforms.CenterCrop(CFG[\"img_size\"]),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    ds_train = ListDataset(train_list, train_tf)\n    ds_val = ListDataset(val_list, val_tf)\n\n    # Training sampler: optional class balancing\n    if CFG[\"balance_train\"]:\n        counts = np.bincount([y for _, y in train_list], minlength=len(class_names))\n        class_weights = 1.0 / np.clip(counts, 1, None)\n        sample_weights = [class_weights[y] for _, y in train_list]\n        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n        train_loader = DataLoader(ds_train, batch_size=CFG[\"batch_size\"], sampler=sampler,\n                                  num_workers=CFG[\"num_workers\"], pin_memory=True)\n    else:\n        train_loader = DataLoader(ds_train, batch_size=CFG[\"batch_size\"], shuffle=True,\n                                  num_workers=CFG[\"num_workers\"], pin_memory=True)\n\n    val_loader = DataLoader(ds_val, batch_size=CFG[\"batch_size\"], shuffle=False,\n                            num_workers=CFG[\"num_workers\"], pin_memory=True)\n    return train_loader, val_loader, class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.819654Z","iopub.execute_input":"2025-09-29T16:47:40.819869Z","iopub.status.idle":"2025-09-29T16:47:40.840819Z","shell.execute_reply.started":"2025-09-29T16:47:40.819852Z","shell.execute_reply":"2025-09-29T16:47:40.840142Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### 5. Model & CAM (Class Activation Mapping) Setup\n#### This cell defines the model architectures (SmallCNN, and logic to adapt ConvNeXt/ResNet). It also includes functions to set up Grad-CAM by automatically selecting a target layer and attaching the necessary forward and backward hooks to capture activations and gradients.","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Model & CAM Setup\n# -----------------------------\nclass SmallCNN(nn.Module):\n    \"\"\"A simple custom CNN for baseline experiments.\"\"\"\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 1, 1), nn.BatchNorm2d(32), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(inplace=True), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d(1)\n        )\n        self.head = nn.Linear(128, num_classes)\n    def forward(self, x):\n        feat = self.conv(x).flatten(1) # B,128,1,1 -> B,128\n        return self.head(feat)\n\ndef build_model(num_classes):\n    \"\"\"Builds the model based on the name specified in CFG.\"\"\"\n    name = CFG[\"model_name\"].lower()\n    if name == \"convnext_tiny\":\n        weights = ConvNeXt_Tiny_Weights.IMAGENET1K_V1 if CFG[\"pretrained\"] else None\n        m = convnext_tiny(weights=weights)\n        m.classifier[-1] = nn.Linear(m.classifier[-1].in_features, num_classes)\n        return m\n    elif name == \"resnet18\":\n        weights = ResNet18_Weights.IMAGENET1K_V1 if CFG[\"pretrained\"] else None\n        m = resnet18(weights=weights)\n        m.fc = nn.Linear(m.fc.in_features, num_classes)\n        return m\n    else:\n        return SmallCNN(num_classes)\n\ndef select_cam_layer(model):\n    \"\"\"Robustly selects a layer for CAM, preferring the last depthwise Conv2d.\"\"\"\n    # Prioritize the last depthwise convolution, common in modern architectures\n    cand = None\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Conv2d) and m.groups > 1 and m.in_channels == m.out_channels:\n            cand = m\n    if cand is not None:\n        return cand\n    # Fallback: take the very last Conv2d layer\n    last = None\n    for n, m in model.named_modules():\n        if isinstance(m, nn.Conv2d):\n            last = m\n    return last\n\ndef build_cam_handle(model):\n    \"\"\"Attaches forward and backward hooks to the target layer for CAM.\"\"\"\n    if not CFG[\"enable_cam\"]:\n        return None, []\n    layer = select_cam_layer(model)\n    if layer is None:\n        print(\"[cam] no suitable conv layer found; CAM disabled.\")\n        return None, []\n    \n    # Dictionaries to store activations and gradients\n    acts, grads = {\"value\": None}, {\"value\": None}\n    \n    def fwd_hook(module, inp, out):\n        acts[\"value\"] = out.detach() if not out.requires_grad else out\n    def bwd_hook(module, grad_in, grad_out):\n        grads[\"value\"] = grad_out[0]\n        \n    h1 = layer.register_forward_hook(fwd_hook)\n    h2 = layer.register_full_backward_hook(bwd_hook)\n    return (acts, grads), [h1, h2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.841673Z","iopub.execute_input":"2025-09-29T16:47:40.841895Z","iopub.status.idle":"2025-09-29T16:47:40.865726Z","shell.execute_reply.started":"2025-09-29T16:47:40.841877Z","shell.execute_reply":"2025-09-29T16:47:40.864878Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### 6. Evaluation & Logging Functions\n#### This set of functions handles the evaluation loop, calculates metrics, and saves various artifacts like the training curve, confusion matrix, per-class classification report, and Grad-CAM visualizations.","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Evaluation & Logging Functions\n# -----------------------------\ndef evaluate(model, loader, num_classes):\n    \"\"\"Evaluates the model on a given dataloader and returns metrics.\"\"\"\n    model.eval()\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for x, y, _ in loader:\n            x = x.to(device)\n            logits = model(x)\n            pred = logits.argmax(1).cpu().numpy().tolist()\n            y_pred.extend(pred)\n            y_true.extend(y.numpy().tolist())\n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred, average=\"macro\")\n    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n    return acc, f1, cm, y_true, y_pred\n\ndef save_cm_and_report(cm, y_true, y_pred, class_names, out_dir: Path, epoch: int):\n    \"\"\"Saves the confusion matrix and a per-class classification report.\"\"\"\n    ensure_dir(out_dir)\n    # Confusion Matrix: write class names in the first row/col for readability\n    cm_path = out_dir / f\"confusion_matrix_epoch{epoch:03d}.csv\"\n    df_cm = pd.DataFrame(cm, columns=class_names)\n    df_cm.insert(0, \"true\\\\pred\", class_names)\n    df_cm.to_csv(cm_path, index=False)\n\n    # Per-class report\n    rep = classification_report(y_true, y_pred, labels=list(range(len(class_names))),\n                                target_names=class_names, output_dict=True, zero_division=0)\n    df_rep = pd.DataFrame(rep).T.reset_index().rename(columns={\"index\": \"class\"})\n    df_rep.to_csv(out_dir / f\"per_class_report_epoch{epoch:03d}.csv\", index=False)\n\ndef save_train_curve(curves, out_csv: Path):\n    \"\"\"Saves the training history (loss, acc, f1) to a CSV file.\"\"\"\n    df = pd.DataFrame(curves)\n    df.to_csv(out_csv, index=False)\n\ndef save_cam_samples(model, loader, class_names, out_dir: Path, actgrad, hooks, max_images=12):\n    \"\"\"Generates and saves Grad-CAM visualization samples.\"\"\"\n    ensure_dir(out_dir / \"cam\")\n    acts, grads = actgrad\n    saved = 0\n    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)\n\n    model.eval()\n    for x, y, paths in loader:\n        # -- Key Fix: Enable gradients for CAM generation --\n        with torch.enable_grad():\n            x = x.to(device)\n            logits = model(x)\n            pred = logits.argmax(1)\n\n            for i in range(x.size(0)):\n                if saved >= max_images: break\n                model.zero_grad(set_to_none=True)\n                score = logits[i, pred[i]]\n                score.backward(retain_graph=True)\n\n                A = acts[\"value\"][i]    # Activations C x H x W\n                dA = grads[\"value\"][i]  # Gradients C x H x W\n                if A.dim() == 4: A = A.squeeze(0)\n                if dA.dim() == 4: dA = dA.squeeze(0)\n\n                # Grad-CAM calculation\n                w = dA.mean(dim=(1, 2), keepdim=True) # C x 1 x 1\n                cam = torch.relu((A * w).sum(dim=0))  # H x W\n                cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n                cam = cam.unsqueeze(0).unsqueeze(0)  # 1x1xHxW\n                cam = F.interpolate(cam, size=x[i:i+1].shape[-2:], mode=\"bilinear\", align_corners=False)[0]\n\n                # Denormalize image for visualization\n                img = x[i:i+1] * std + mean\n                img = img.clamp(0, 1)[0]\n\n                # Create simple red heatmap and overlay it on the image\n                heat = torch.zeros_like(img)\n                heat[0] = cam[0]\n                overlay = (0.6 * img + 0.4 * heat).clamp(0, 1)\n\n                # Save the result\n                name = f\"{saved:02d}_pred_{class_names[pred[i]]}.jpg\"\n                torchvision.utils.save_image(overlay.cpu(), out_dir / \"cam\" / name)\n                saved += 1\n        if saved >= max_images: break\n    \n    # Clean up hooks to prevent memory leaks\n    for h in hooks:\n        h.remove()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.866619Z","iopub.execute_input":"2025-09-29T16:47:40.866849Z","iopub.status.idle":"2025-09-29T16:47:40.885286Z","shell.execute_reply.started":"2025-09-29T16:47:40.866830Z","shell.execute_reply":"2025-09-29T16:47:40.884558Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### 7. Main Training Pipeline","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Main Training Pipeline\n# -----------------------------\ndef train_one_run():\n    \"\"\"Executes a full training and validation run.\"\"\"\n    set_seed(CFG[\"seed\"])\n    out_dir = Path(CFG[\"out_root\"]) / CFG[\"run_name\"]\n    met_dir = out_dir / \"metrics\"\n    ensure_dir(met_dir)\n\n    train_loader, val_loader, class_names = build_dataloaders()\n    num_classes = len(class_names)\n    print(f\"Starting training for {num_classes} classes.\")\n\n    # Model / Optimizer / Criterion\n    model = build_model(num_classes).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG[\"lr\"], weight_decay=CFG[\"weight_decay\"])\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG[\"mixed_precision\"])\n    criterion = nn.CrossEntropyLoss()\n\n    # Save resource info\n    with open(met_dir / \"resource.json\", \"w\") as f:\n        json.dump({\n            \"params\": sum(p.numel() for p in model.parameters()),\n            \"model\": CFG[\"model_name\"], \"pretrained\": CFG[\"pretrained\"],\n            \"img_size\": CFG[\"img_size\"], \"batch_size\": CFG[\"batch_size\"],\n            \"num_classes\": num_classes, \"class_names\": class_names,\n        }, f, indent=2)\n\n    # Setup CAM hooks\n    actgrad, hooks = build_cam_handle(model)\n\n    best_f1, best_ep = -1.0, -1\n    # Columns: loss=train_loss, acc=train_acc, f1=val_f1(macro)\n    curves = {\"epoch\": [], \"loss\": [], \"acc\": [], \"f1\": []}\n    best_path = out_dir / f\"{CFG['run_name']}_best.pt\"\n\n    for epoch in range(1, CFG[\"epochs\"] + 1):\n        model.train()\n        losses, preds, gts = [], [], []\n        \n        for x, y, _ in train_loader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=CFG[\"mixed_precision\"]):\n                logits = model(x)\n                loss = criterion(logits, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            losses.append(loss.item())\n            preds.extend(logits.argmax(1).detach().cpu().numpy().tolist())\n            gts.extend(y.detach().cpu().numpy().tolist())\n\n        tr_loss = float(np.mean(losses))\n        tr_acc = accuracy_score(gts, preds)\n\n        # Validation\n        val_acc, val_f1, cm, y_true, y_pred = evaluate(model, val_loader, num_classes)\n        print(f\"[{CFG['model_name']}] [Epoch {epoch:03d}/{CFG['epochs']}] \"\n              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_acc={val_acc:.4f} f1={val_f1:.4f}\")\n\n        # Log metrics for the training curve\n        curves[\"epoch\"].append(epoch)\n        curves[\"loss\"].append(tr_loss)\n        curves[\"acc\"].append(tr_acc)\n        curves[\"f1\"].append(val_f1)\n        save_train_curve(curves, met_dir / \"train_curve.csv\")\n\n        # Save confusion matrix & report (periodically)\n        if epoch % CFG[\"save_every\"] == 0 or epoch == CFG[\"epochs\"]:\n            save_cm_and_report(cm, y_true, y_pred, class_names, met_dir, epoch)\n\n        # Save CAM samples on the first epoch\n        if CFG[\"enable_cam\"] and epoch == 1 and actgrad is not None:\n            save_cam_samples(model, val_loader, class_names, met_dir, actgrad, hooks, CFG[\"cam_max_images\"])\n\n        # Save the best model based on validation F1 score\n        if val_f1 > best_f1:\n            best_f1, best_ep = val_f1, epoch\n            torch.save({\"model_cfg\": CFG, \"state_dict\": model.state_dict(), \"class_names\": class_names},\n                       best_path)\n            print(f\"  -> New best model saved at epoch {best_ep} with F1-score: {best_f1:.4f}\")\n\n    print(f\"\\nTraining finished. Best model from epoch {best_ep} saved at {best_path}\")\n    return out_dir, best_ep, class_names, best_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.886105Z","iopub.execute_input":"2025-09-29T16:47:40.886324Z","iopub.status.idle":"2025-09-29T16:47:40.908943Z","shell.execute_reply.started":"2025-09-29T16:47:40.886289Z","shell.execute_reply":"2025-09-29T16:47:40.908266Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### 8. Fine-tuning at a Higher Resolution","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# (Optional) Fine-tuning at 288px\n# -----------------------------\ndef finetune_288(best_ckpt_path: Path, class_names):\n    \"\"\"(Optional) Fine-tunes the best model at a higher resolution (288px).\"\"\"\n    print(\"\\n--- Starting 288px Fine-tuning Stage ---\")\n    num_classes = len(class_names)\n    \n    # Load the best model from the previous stage\n    model = build_model(num_classes).to(device)\n    ckpt = torch.load(best_ckpt_path, map_location=\"cpu\")\n    model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n\n    # Build new dataloaders with 288px image size and light augmentation\n    data_root = auto_find_data_root()\n    files_by_class = build_index(data_root, CFG[\"use_subdirs\"])\n    train_list, val_list, _ = stratified_split_by_class(files_by_class, CFG[\"split_ratio\"])\n    \n    train_tf = transforms.Compose([\n        transforms.RandomResizedCrop(288, scale=(0.7, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    val_tf = transforms.Compose([\n        transforms.Resize(288 + 32),\n        transforms.CenterCrop(288),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    \n    ds_train = ListDataset(train_list, train_tf)\n    ds_val = ListDataset(val_list, val_tf)\n    train_loader = DataLoader(ds_train, batch_size=48, shuffle=True, num_workers=CFG[\"num_workers\"], pin_memory=True)\n    val_loader = DataLoader(ds_val, batch_size=64, shuffle=False, num_workers=CFG[\"num_workers\"], pin_memory=True)\n\n    # Fine-tune with a low learning rate\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-5)\n    scaler = torch.cuda.amp.GradScaler(enabled=CFG[\"mixed_precision\"])\n    criterion = nn.CrossEntropyLoss()\n\n    out_dir = Path(CFG[\"out_root\"]) / (CFG[\"run_name\"] + \"_ft288\")\n    met_dir = out_dir / \"metrics\"\n    ensure_dir(met_dir)\n\n    best_f1, best_ep = -1.0, -1\n    for epoch in range(1, 6):\n        model.train()\n        losses, preds, gts = [], [], []\n        for x, y, _ in train_loader:\n            x, y = x.to(device), y.to(device)\n            optimizer.zero_grad(set_to_none=True)\n            with torch.cuda.amp.autocast(enabled=CFG[\"mixed_precision\"]):\n                logits = model(x)\n                loss = criterion(logits, y)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n            losses.append(loss.item())\n            preds.extend(logits.argmax(1).detach().cpu().numpy().tolist())\n            gts.extend(y.detach().cpu().numpy().tolist())\n\n        tr_loss = float(np.mean(losses))\n        tr_acc = accuracy_score(gts, preds)\n        val_acc, val_f1, cm, y_true, y_pred = evaluate(model, val_loader, num_classes)\n\n        print(f\"[FT 288] [Epoch {epoch}/5] train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n              f\"val_acc={val_acc:.4f} f1={val_f1:.4f}\")\n\n        if val_f1 > best_f1:\n            best_f1, best_ep = val_f1, epoch\n            best_ft_path = out_dir / f\"{CFG['run_name']}_ft288_best.pt\"\n            torch.save({\"model_cfg\": CFG, \"state_dict\": model.state_dict(), \"class_names\": class_names},\n                       best_ft_path)\n            print(f\"  -> New best fine-tuned model saved at epoch {best_ep} with F1-score: {best_f1:.4f}\")\n        \n        save_cm_and_report(cm, y_true, y_pred, class_names, met_dir, epoch)\n\n    print(f\"\\nFine-tuning finished. Best model saved at {best_ft_path}\")\n    return out_dir, best_ep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.909862Z","iopub.execute_input":"2025-09-29T16:47:40.910152Z","iopub.status.idle":"2025-09-29T16:47:40.935189Z","shell.execute_reply.started":"2025-09-29T16:47:40.910127Z","shell.execute_reply":"2025-09-29T16:47:40.934450Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -----------------------------\n# Execution\n# -----------------------------\nset_seed(CFG[\"seed\"])\nensure_dir(Path(CFG[\"out_root\"]))\nprint(\"Using device:\", device)\n\n# --- Start the main training run ---\nout_dir, best_ep, class_names, best_path = train_one_run()\n\n# --- (Optional) Uncomment the lines below to run the 288px fine-tuning stage ---\n# print(\"\\n\" + \"=\"*50)\n# ft_out_dir, ft_best_ep = finetune_288(best_path, class_names)\n# print(\"=\"*50)\n\nprint(f\"\\nAll done. Main outputs are under: {out_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:47:40.935995Z","iopub.execute_input":"2025-09-29T16:47:40.936212Z","execution_failed":"2025-09-29T16:49:36.874Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nStarting training for 50 classes.\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n100%|██████████| 109M/109M [00:00<00:00, 164MB/s]  \n","output_type":"stream"}],"execution_count":null}]}