{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7880,"databundleVersionId":862031,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# Large → Seedlings 迁移实验（放在新 cell 里）\n# - 载入你在大数据集上训练得到的权重（ckpt）\n# - 两种策略：\n#     1) full_ft     ：全量微调\n#     2) lp_unfreeze ：线性探针(冻结backbone) → 逐步解冻\n# - 模型支持：convnext_tiny / resnet18\n# - 产物：metrics/train_curve.csv、混淆矩阵 & per-class report、最佳权重\n# ============================================\n\nimport os, math, random, time, json, warnings\nfrom pathlib import Path\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport torchvision\nfrom torchvision import transforms as T, models\n\n# ---------------------------\n# 用户需在此处手动设置\n# ---------------------------\nCFG = {\n    \"seed\": 42,\n    \"strategy\": \"full_ft\",        # \"full_ft\" | \"lp_unfreeze\"\n    \"model_name\": \"convnext_tiny\",# \"convnext_tiny\" | \"resnet18\"\n    \"ckpt_path\": \"/kaggle/working/leafsnap_runs/convnext_tiny_leafsnap_phase4/convnext_tiny_leafsnap_phase4_best.pt\",\n    # ^^^ 上面改成你“大数据集训练阶段”的 best 权重路径（*.pt）\n\n    # 数据路径（Kaggle 官方 Plant Seedlings）\n    \"data_root\": \"/kaggle/input/plant-seedlings-classification\",\n    \"val_ratio\": 0.1,\n\n    # 训练配置\n    \"img_size\": 224,\n    \"batch_size\": 128,\n    \"num_workers\": 2,\n    \"epochs_fullft\": 30,          # 全量微调的训练轮数\n    \"epochs_lp\": 5,               # 线性探针阶段轮数（冻结backbone，仅训分类头）\n    \"epochs_unfreeze\": 15,        # 逐步解冻阶段轮数\n    \"lr_head\": 1e-3,              # 分类头学习率\n    \"lr_backbone\": 3e-4,          # backbone 学习率（full_ft 或解冻阶段使用）\n    \"weight_decay\": 1e-2,\n    \"label_smoothing\": 0.1,\n    \"mixed_precision\": True,\n\n    \"out_root\": \"/kaggle/working/seedlings_transfer\",\n    \"run_name\": None,             # 若为 None 自动根据策略/模型命名\n}\n\n# ---------------------------\n# 基础工具\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = True\n\ndef ensure_dir(p: Path):\n    p.mkdir(parents=True, exist_ok=True)\n\ndef build_seedlings_split(root: Path, val_ratio=0.1, seed=42):\n    train_dir = root/\"train\"\n    assert train_dir.exists(), f\"Plant Seedlings 数据不存在：{train_dir}\"\n    classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n    c2i = {c:i for i,c in enumerate(classes)}\n    items=[]\n    exts={\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n    for c in classes:\n        for p in (train_dir/c).glob(\"*.*\"):\n            if p.suffix.lower() in exts:\n                items.append((p, c2i[c]))\n    y = np.array([b for _,b in items]); idx = np.arange(len(items))\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n    tr, va = next(sss.split(idx,y))\n    return classes, [items[i] for i in tr], [items[i] for i in va]\n\nclass DS(Dataset):\n    def __init__(self, items, tfm): self.items, self.tfm = items, tfm\n    def __len__(self): return len(self.items)\n    def __getitem__(self, i):\n        p, y = self.items[i]\n        img = Image.open(p).convert(\"RGB\")\n        x = self.tfm(img)\n        return x, y, str(p)\n\ndef get_tfms(size):\n    train = T.Compose([\n        T.Resize(int(size*1.14)),\n        T.RandomResizedCrop(size, scale=(0.85,1.0), ratio=(3/4,4/3)),\n        T.RandomHorizontalFlip(0.5),\n        T.RandAugment(2, 9),\n        T.ToTensor(),\n        T.RandomErasing(p=0.1),\n        T.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n    ])\n    valid = T.Compose([\n        T.Resize(int(size*1.14)),\n        T.CenterCrop(size),\n        T.ToTensor(),\n        T.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225)),\n    ])\n    return train, valid\n\ndef accuracy(out, tgt):\n    with torch.no_grad():\n        return (out.argmax(1)==tgt).float().mean().item()\n\n@torch.no_grad()\ndef evaluate(model, loader, num_classes):\n    model.eval()\n    all_pred=[]; all_true=[]; loss_sum=acc_sum=n=0\n    crit = nn.CrossEntropyLoss()\n    for x,y,_ in loader:\n        x=x.to(device); y=y.to(device)\n        with torch.amp.autocast(\"cuda\", enabled=CFG[\"mixed_precision\"]):\n            lo = model(x); ls = crit(lo,y)\n        bs=x.size(0)\n        loss_sum += ls.item()*bs\n        acc_sum  += accuracy(lo,y)*bs\n        n += bs\n        all_pred.append(lo.argmax(1).cpu().numpy())\n        all_true.append(y.cpu().numpy())\n    pred = np.concatenate(all_pred); true = np.concatenate(all_true)\n    f1 = f1_score(true, pred, average=\"macro\")\n    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n    return loss_sum/n, acc_sum/n, f1, cm, true, pred\n\ndef save_cm_and_report(cm, y_true, y_pred, class_names, out_dir: Path, epoch: int):\n    ensure_dir(out_dir)\n    df_cm = pd.DataFrame(cm, columns=class_names)\n    df_cm.insert(0, \"true\\\\pred\", class_names)\n    df_cm.to_csv(out_dir / f\"confusion_matrix_epoch{epoch:03d}.csv\", index=False)\n\n    rep = classification_report(y_true, y_pred, labels=list(range(len(class_names))),\n                                target_names=class_names, output_dict=True, zero_division=0)\n    pd.DataFrame(rep).T.reset_index().rename(columns={\"index\":\"class\"}).to_csv(\n        out_dir / f\"per_class_report_epoch{epoch:03d}.csv\", index=False\n    )\n\ndef save_curve(curves, out_csv: Path):\n    pd.DataFrame(curves).to_csv(out_csv, index=False)\n\n# ---------------------------\n# 模型与“只加载 backbone”工具\n# ---------------------------\ndef build_model(num_classes, name=\"convnext_tiny\"):\n    n = name.lower()\n    if n == \"convnext_tiny\":\n        m = models.convnext_tiny(weights=None)\n        m.classifier[2] = nn.Linear(m.classifier[2].in_features, num_classes)\n        return m\n    elif n == \"resnet18\":\n        m = models.resnet18(weights=None)\n        m.fc = nn.Linear(m.fc.in_features, num_classes)\n        return m\n    else:\n        raise ValueError(f\"Unsupported model_name: {name}\")\n\ndef load_backbone_from_ckpt(model, ckpt_path, model_name):\n    \"\"\"\n    仅加载 backbone（丢弃旧分类头）。ckpt 需是 {'state_dict': ..., ...} 或纯 state_dict。\n    \"\"\"\n    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n    sd = ckpt.get(\"state_dict\", ckpt)\n    new_sd = model.state_dict()\n    drop_keys = []\n    if model_name==\"convnext_tiny\":\n        # 丢弃 classifier.*（最后线性层等）\n        drop_prefixes = [\"classifier.\"]\n    else:  # resnet18\n        drop_prefixes = [\"fc.\"]\n    for k in list(sd.keys()):\n        if any(k.startswith(p) for p in drop_prefixes):\n            drop_keys.append(k)\n            sd.pop(k)\n    # 过滤形状不匹配的键\n    matched = {k:v for k,v in sd.items() if (k in new_sd and new_sd[k].shape == v.shape)}\n    missing = [k for k in new_sd.keys() if k not in matched]\n    print(f\"[ckpt] load backbone: matched={len(matched)} | missing(new head etc.)={len(missing)} | dropped_old_head={len(drop_keys)}\")\n    model.load_state_dict({**new_sd, **matched})\n    return model\n\n# ---------------------------\n# 冻结/解冻工具（BN 细节）\n# ---------------------------\ndef set_backbone_trainable(model, model_name, trainable: bool):\n    if model_name==\"convnext_tiny\":\n        backbone = [model.features]  # convnext features 模块\n    else:\n        backbone = [nn.Sequential(model.conv1, model.bn1, model.layer1, model.layer2, model.layer3, model.layer4)]\n    for m in backbone:\n        for p in m.parameters(): p.requires_grad = trainable\n        # BN：冻结时 eval，解冻时 train\n        for mm in m.modules():\n            if isinstance(mm, (nn.BatchNorm2d, nn.LayerNorm)):\n                mm.eval() if not trainable else mm.train()\n\ndef unfreeze_last_stages(model, model_name, stages=1):\n    \"\"\"\n    只对解冻阶段有用：逐步解冻最后若干 stage。\n    \"\"\"\n    set_backbone_trainable(model, model_name, trainable=False)  # 先全冻\n    if model_name==\"convnext_tiny\":\n        blocks = model.features  # Sequential\n        # convnext_tiny stages roughly: [0..6] stem+stages；最后若干层解冻\n        to_unfreeze = list(range(len(blocks)-stages, len(blocks)))\n        for i in to_unfreeze:\n            for p in blocks[i].parameters(): p.requires_grad=True\n            for mm in blocks[i].modules():\n                if isinstance(mm, (nn.BatchNorm2d, nn.LayerNorm)): mm.train()\n    else:\n        # resnet18: layer1/2/3/4（解冻最后 n 个）\n        layers = [model.layer1, model.layer2, model.layer3, model.layer4]\n        for l in layers[-stages:]:\n            for p in l.parameters(): p.requires_grad=True\n            for mm in l.modules():\n                if isinstance(mm, nn.BatchNorm2d): mm.train()\n\n# ---------------------------\n# 优化器 & 训练循环\n# ---------------------------\ndef make_optimizer(model, model_name, lr_backbone, lr_head, wd=1e-2, full=False):\n    if model_name==\"convnext_tiny\":\n        head_params = list(model.classifier.parameters())\n        bb_params   = list(model.features.parameters())\n    else:\n        head_params = list(model.fc.parameters())\n        bb_params   = [p for n,p in model.named_parameters() if not n.startswith(\"fc.\")]\n    params=[]\n    if full:\n        # discriminative LR：backbone 用较小 lr\n        params=[{\"params\": bb_params, \"lr\": lr_backbone},\n                {\"params\": head_params, \"lr\": lr_head}]\n    else:\n        params=[{\"params\": head_params, \"lr\": lr_head}]\n    return torch.optim.AdamW(params, weight_decay=wd)\n\ndef train_one_phase(model, train_loader, valid_loader, epochs, opt, crit, curves, out_met_dir, class_names, start_epoch=1):\n    scaler = torch.amp.GradScaler(\"cuda\", enabled=CFG[\"mixed_precision\"])\n    best_f1=-1; best_ep=-1; best_path = out_met_dir.parent / f\"{run_name}_best.pt\"\n    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1,epochs))\n    for e in range(start_epoch, start_epoch+epochs):\n        model.train(); t0=time.time()\n        loss_sum=acc_sum=n=0\n        for x,y,_ in train_loader:\n            x=x.to(device); y=y.to(device)\n            opt.zero_grad(set_to_none=True)\n            with torch.amp.autocast(\"cuda\", enabled=CFG[\"mixed_precision\"]):\n                lo = model(x)\n                ls = crit(lo,y)\n            scaler.scale(ls).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(opt); scaler.update()\n            loss_sum += ls.item()*x.size(0)\n            acc_sum  += accuracy(lo,y)*x.size(0)\n            n += x.size(0)\n        sched.step()\n\n        tr_loss, tr_acc = loss_sum/n, acc_sum/n\n        va_loss, va_acc, va_f1, cm, y_true, y_pred = evaluate(model, valid_loader, num_classes=len(class_names))\n        print(f\"[{run_name}] Epoch {e:02d}/{start_epoch+epochs-1:02d} | \"\n              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | val_loss={va_loss:.4f} acc={va_acc:.4f} f1={va_f1:.4f} | time={int(time.time()-t0)}s\")\n\n        curves[\"epoch\"].append(e)\n        curves[\"train_loss\"].append(tr_loss)\n        curves[\"train_acc\"].append(tr_acc)\n        curves[\"val_acc\"].append(va_acc)\n        curves[\"val_f1\"].append(va_f1)\n        save_curve(curves, out_met_dir/\"train_curve.csv\")\n        save_cm_and_report(cm, y_true, y_pred, class_names, out_met_dir, e)\n\n        if va_f1 > best_f1:\n            best_f1, best_ep = va_f1, e\n            torch.save({\"state_dict\": model.state_dict(), \"classes\": class_names}, best_path)\n    return best_f1, best_ep, best_path\n\n# ---------------------------\n# 主流程\n# ---------------------------\nset_seed(CFG[\"seed\"])\nroot = Path(CFG[\"data_root\"])\nclasses, tr_items, va_items = build_seedlings_split(root, CFG[\"val_ratio\"], CFG[\"seed\"])\nnum_classes = len(classes)\ntrain_tfm, valid_tfm = get_tfms(CFG[\"img_size\"])\ntrain_ds, valid_ds = DS(tr_items, train_tfm), DS(va_items, valid_tfm)\ntrain_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True)\n\n# 命名与输出目录\nif CFG[\"run_name\"] is None:\n    run_name = f\"seedlings_{CFG['model_name']}_{CFG['strategy']}\"\nelse:\n    run_name = CFG[\"run_name\"]\nout_dir = Path(CFG[\"out_root\"])/run_name\nmet_dir = out_dir/\"metrics\"\nensure_dir(met_dir)\n\n# 模型：新建 Seedlings 头，然后只加载 backbone 参数\nmodel = build_model(num_classes, CFG[\"model_name\"]).to(device)\nmodel = load_backbone_from_ckpt(model, CFG[\"ckpt_path\"], CFG[\"model_name\"])\n\n# 写资源信息\nwith open(met_dir/\"resource.json\", \"w\") as f:\n    json.dump({\n        \"strategy\": CFG[\"strategy\"],\n        \"model\": CFG[\"model_name\"],\n        \"img_size\": CFG[\"img_size\"],\n        \"batch_size\": CFG[\"batch_size\"],\n        \"params\": sum(p.numel() for p in model.parameters())\n    }, f)\n\ncurves = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_acc\": [], \"val_f1\": []}\ncrit = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smoothing\"])\n\nif CFG[\"strategy\"] == \"full_ft\":\n    # 全量微调：backbone 可训练；分组LR（backbone用较小lr）\n    set_backbone_trainable(model, CFG[\"model_name\"], trainable=True)\n    opt = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                         wd=CFG[\"weight_decay\"], full=True)\n    best_f1, best_ep, best_path = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_fullft\"], opt, crit, curves, met_dir, classes\n    )\n\nelif CFG[\"strategy\"] == \"lp_unfreeze\":\n    # STEP1 线性探针：冻结 backbone，只训 head\n    set_backbone_trainable(model, CFG[\"model_name\"], trainable=False)\n    opt = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                         wd=CFG[\"weight_decay\"], full=False)\n    best_f1, best_ep, best_path = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_lp\"], opt, crit, curves, met_dir, classes, start_epoch=1\n    )\n\n    # STEP2 逐步解冻：解冻最后 n 个 stage（ConvNeXt 默认 1，ResNet 默认 1~2）\n    unfreeze_last_stages(model, CFG[\"model_name\"], stages=1 if CFG[\"model_name\"]==\"convnext_tiny\" else 2)\n    opt = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                         wd=CFG[\"weight_decay\"], full=True)\n    b2_f1, b2_ep, best_path = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_unfreeze\"], opt, crit, curves, met_dir, classes,\n        start_epoch=(CFG[\"epochs_lp\"]+1)\n    )\n    if b2_f1 > best_f1:\n        best_f1, best_ep = b2_f1, b2_ep\n\nelse:\n    raise ValueError(\"CFG['strategy'] 必须是 'full_ft' 或 'lp_unfreeze'\")\n\nprint(f\"\\nDone. Best macro-F1={best_f1:.4f} @ epoch {best_ep}.\")\nprint(\"Outputs ->\", out_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Experiment 3 – In-Domain vs. Generic Transfer Learning\n\n#### This experiment investigates whether pretraining on a large, in-domain dataset (LeafSnap) offers better initialization for a smaller target task (Plant Seedlings) than generic ImageNet pretraining. It compares full fine-tuning and gradual unfreezing strategies to determine which transfer learning approach maximizes performance","metadata":{}},{"cell_type":"markdown","source":"### 1. Configuration\n#### This cell contains all the hyperparameters and settings for the experiment in a single dictionary CFG. ","metadata":{}},{"cell_type":"code","source":"\nCFG = {\n    # --- General ---\n    \"seed\": 42,\n    \"strategy\": \"lp_unfreeze\",       # Options: \"full_ft\" | \"lp_unfreeze\"\n    \"model_name\": \"convnext_tiny\",    # Options: \"convnext_tiny\" | \"resnet18\"\n    \n    # --- Paths ---\n    # Path to the best weights from your pre-training on the large dataset\n    \"ckpt_path\": \"/kaggle/working/leafsnap_runs/convnext_tiny_leafsnap_phase4/convnext_tiny_leafsnap_phase4_best.pt\",\n    # Path to the Plant Seedlings dataset\n    \"data_root\": \"/kaggle/input/plant-seedlings-classification\",\n    # Root directory for saving outputs (weights, logs, metrics)\n    \"out_root\": \"/kaggle/working/seedlings_transfer_convnext_tiny\",\n    # Name for this specific run. If None, it's auto-generated.\n    \"run_name\": None,                 \n\n    # --- Data & Splitting ---\n    \"val_ratio\": 0.1,                 # Ratio of data to be used for validation\n\n    # --- Training Parameters ---\n    \"img_size\": 224,\n    \"batch_size\": 128,\n    \"num_workers\": 2,\n    \"epochs_fullft\": 30,              # Number of epochs for the \"full fine-tuning\" strategy\n    \"epochs_lp\": 5,                   # Number of epochs for the \"linear probing\" phase\n    \"epochs_unfreeze\": 15,            # Number of epochs for the \"gradual unfreezing\" phase\n    \"lr_head\": 1e-3,                  # Learning rate for the classification head\n    \"lr_backbone\": 3e-4,              # Learning rate for the backbone (used in full_ft or unfreeze phase)\n    \"weight_decay\": 1e-2,\n    \"label_smoothing\": 0.1,\n    \"mixed_precision\": True,          # Enable/disable Automatic Mixed Precision (AMP)\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Imports\n#### This cell imports all the necessary libraries for the project, including PyTorch, Torchvision, NumPy, Pandas, and Scikit-learn.","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Imports and Environment Setup\n# =========================================================================================\nimport os, math, random, time, json, warnings\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nimport torchvision\nfrom torchvision import transforms as T, models\n\n# Suppress warnings for a cleaner output\nwarnings.filterwarnings(\"ignore\")\n\n# Set the device to CUDA if available, otherwise CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Core Utilities\n#### This section contains essential helper functions for reproducibility, file handling, and data splitting.","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Core Utility Functions\n# =========================================================================================\n\ndef set_seed(seed=42):\n    \"\"\"Sets the random seed for reproducibility across all relevant libraries.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    # Ensures that CUDA selects the same algorithm each time, which can be a minor source of non-determinism\n    torch.backends.cudnn.benchmark = True\n\ndef ensure_dir(p: Path):\n    \"\"\"Creates a directory if it does not already exist.\"\"\"\n    p.mkdir(parents=True, exist_ok=True)\n\ndef build_seedlings_split(root: Path, val_ratio=0.1, seed=42):\n    \"\"\"\n    Scans the Plant Seedlings dataset directory and creates a stratified train/validation split.\n    Returns: A tuple containing (class_names, train_items, validation_items).\n    \"\"\"\n    train_dir = root / \"train\"\n    assert train_dir.exists(), f\"Plant Seedlings data not found at: {train_dir}\"\n    \n    # Get class names from subdirectories\n    classes = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n    class_to_idx = {c: i for i, c in enumerate(classes)}\n    \n    # Collect all image paths and their corresponding labels\n    items = []\n    valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n    for c in classes:\n        for p in (train_dir / c).glob(\"*.*\"):\n            if p.suffix.lower() in valid_exts:\n                items.append((p, class_to_idx[c]))\n    \n    # Perform stratified split\n    labels = np.array([label for _, label in items])\n    indices = np.arange(len(items))\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n    train_idx, val_idx = next(sss.split(indices, labels))\n    \n    train_items = [items[i] for i in train_idx]\n    val_items = [items[i] for i in val_idx]\n    \n    return classes, train_items, val_items","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Data Handling\n#### Here, we define the PyTorch Dataset class to load images and the transforms for data augmentation and normalization.","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Data Preparation: Dataset Class and Image Transforms\n# =========================================================================================\n\nclass PlantDataset(Dataset):\n    \"\"\"Custom PyTorch Dataset for loading plant seedling images.\"\"\"\n    def __init__(self, items, transform):\n        self.items = items\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.items)\n    \n    def __getitem__(self, i):\n        path, label = self.items[i]\n        image = Image.open(path).convert(\"RGB\")\n        image_tensor = self.transform(image)\n        return image_tensor, label, str(path)\n\ndef get_transforms(img_size):\n    \"\"\"\n    Defines the image augmentation and normalization pipelines for training and validation.\n    \"\"\"\n    # Augmentations for the training set\n    train_transform = T.Compose([\n        T.Resize(int(img_size * 1.14)),\n        T.RandomResizedCrop(img_size, scale=(0.85, 1.0), ratio=(3./4., 4./3.)),\n        T.RandomHorizontalFlip(p=0.5),\n        T.RandAugment(num_ops=2, magnitude=9),\n        T.ToTensor(),\n        T.RandomErasing(p=0.1),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    # Simpler transforms for the validation set (no augmentation)\n    valid_transform = T.Compose([\n        T.Resize(int(img_size * 1.14)),\n        T.CenterCrop(img_size),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    \n    return train_transform, valid_transform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Model Architecture","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Model Definition and Pre-trained Backbone Loader\n# =========================================================================================\n\ndef build_model(num_classes, model_name=\"convnext_tiny\"):\n    \"\"\"\n    Builds a model with a new classification head adapted for the number of target classes.\n    \"\"\"\n    name_lower = model_name.lower()\n    if name_lower == \"convnext_tiny\":\n        model = models.convnext_tiny(weights=None)\n        # Replace the final layer\n        in_features = model.classifier[2].in_features\n        model.classifier[2] = nn.Linear(in_features, num_classes)\n    elif name_lower == \"resnet18\":\n        model = models.resnet18(weights=None)\n        # Replace the final fully connected layer\n        in_features = model.fc.in_features\n        model.fc = nn.Linear(in_features, num_classes)\n    else:\n        raise ValueError(f\"Unsupported model_name: {model_name}\")\n    return model\n\ndef load_backbone_from_ckpt(model, ckpt_path, model_name):\n    \"\"\"\n    Loads only the backbone weights from a checkpoint, ignoring the classification head.\n    The checkpoint can be a full dictionary {'state_dict': ...} or just the state_dict itself.\n    \"\"\"\n    # Load the checkpoint\n    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n    state_dict = ckpt.get(\"state_dict\", ckpt)\n    \n    # Get the state_dict of the new model\n    new_state_dict = model.state_dict()\n    \n    # Define prefixes for the classification head layers to be dropped\n    if model_name.lower() == \"convnext_tiny\":\n        drop_prefixes = (\"classifier.\",)\n    else:  # Assumes resnet18\n        drop_prefixes = (\"fc.\",)\n        \n    # Remove the old classification head weights from the loaded state_dict\n    dropped_keys = []\n    for k in list(state_dict.keys()):\n        if k.startswith(drop_prefixes):\n            dropped_keys.append(state_dict.pop(k))\n    \n    # Filter for weights that match in name and shape\n    matched_state_dict = {k: v for k, v in state_dict.items() \n                          if k in new_state_dict and new_state_dict[k].shape == v.shape}\n    \n    missing_keys = [k for k in new_state_dict.keys() if k not in matched_state_dict]\n    \n    print(f\"[CKPT INFO] Loading backbone weights:\")\n    print(f\"  - Matched layers: {len(matched_state_dict)}\")\n    print(f\"  - Missing (new head, etc.): {len(missing_keys)}\")\n    print(f\"  - Dropped (old head): {len(dropped_keys)}\")\n    \n    # Load the matched weights into the new model\n    model.load_state_dict({**new_state_dict, **matched_state_dict})\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T16:44:53.525217Z","iopub.execute_input":"2025-09-29T16:44:53.525456Z","iopub.status.idle":"2025-09-29T16:44:53.539493Z","shell.execute_reply.started":"2025-09-29T16:44:53.525435Z","shell.execute_reply":"2025-09-29T16:44:53.538401Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### 6. Fine-Tuning & Layer Freezing Utilities","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Model Freezing/Unfreezing Utilities\n# =========================================================================================\n\ndef set_backbone_trainable(model, model_name, trainable: bool):\n    \"\"\"\n    Sets the `requires_grad` property for all parameters in the model's backbone.\n    Also handles setting BatchNorm/LayerNorm layers to train or eval mode accordingly.\n    \"\"\"\n    if model_name.lower() == \"convnext_tiny\":\n        backbone_modules = [model.features]\n    else: # Assumes resnet18\n        backbone_modules = [\n            nn.Sequential(model.conv1, model.bn1, model.relu, model.maxpool, \n                          model.layer1, model.layer2, model.layer3, model.layer4)\n        ]\n\n    for module in backbone_modules:\n        # Set parameter trainability\n        for param in module.parameters():\n            param.requires_grad = trainable\n        # Set norm layers to the correct mode\n        for norm_layer in module.modules():\n            if isinstance(norm_layer, (nn.BatchNorm2d, nn.LayerNorm)):\n                if trainable:\n                    norm_layer.train()\n                else:\n                    norm_layer.eval()\n\ndef unfreeze_last_stages(model, model_name, stages_to_unfreeze=1):\n    \"\"\"\n    Unfreezes the last N stages of the backbone for fine-tuning.\n    This is used in the gradual unfreezing phase.\n    \"\"\"\n    # First, freeze the entire backbone\n    set_backbone_trainable(model, model_name, trainable=False)\n    \n    print(f\"Unfreezing the last {stages_to_unfreeze} stage(s) of the {model_name} backbone...\")\n    \n    # Identify and unfreeze the target layers\n    if model_name.lower() == \"convnext_tiny\":\n        # The 'features' module is a Sequential list of blocks.\n        # Stages are roughly groups of these blocks. We unfreeze the last few blocks.\n        all_blocks = model.features\n        target_blocks = list(all_blocks)[-stages_to_unfreeze:]\n    else: # Assumes resnet18\n        layers = [model.layer1, model.layer2, model.layer3, model.layer4]\n        target_blocks = layers[-stages_to_unfreeze:]\n\n    # Unfreeze parameters and set norm layers to train mode for the target blocks\n    for block in target_blocks:\n        for param in block.parameters():\n            param.requires_grad = True\n        for module in block.modules():\n            if isinstance(module, (nn.BatchNorm2d, nn.LayerNorm)):\n                module.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Training & Evaluation","metadata":{}},{"cell_type":"code","source":"# =========================================================================================\n# Optimizer, Training Loop, and Evaluation\n# =========================================================================================\n\ndef make_optimizer(model, model_name, lr_backbone, lr_head, wd=1e-2, full_ft=False):\n    \"\"\"\n    Creates an AdamW optimizer with differential learning rates for the head and backbone.\n    \"\"\"\n    if model_name.lower() == \"convnext_tiny\":\n        head_params = list(model.classifier.parameters())\n        backbone_params = list(model.features.parameters())\n    else: # Assumes resnet18\n        head_params = list(model.fc.parameters())\n        backbone_params = [p for n, p in model.named_parameters() if not n.startswith(\"fc.\")]\n    \n    param_groups = []\n    # If full fine-tuning, create two parameter groups with different learning rates\n    if full_ft:\n        param_groups.append({\"params\": backbone_params, \"lr\": lr_backbone})\n        param_groups.append({\"params\": head_params, \"lr\": lr_head})\n    # Otherwise, only train the head\n    else:\n        param_groups.append({\"params\": head_params, \"lr\": lr_head})\n        \n    return torch.optim.AdamW(param_groups, weight_decay=wd)\n\n@torch.no_grad()\ndef evaluate(model, loader, num_classes):\n    \"\"\"Evaluates the model on a given dataloader and computes metrics.\"\"\"\n    model.eval()\n    all_preds, all_trues = [], []\n    loss_sum = acc_sum = n = 0\n    criterion = nn.CrossEntropyLoss()\n    \n    for x, y, _ in loader:\n        x, y = x.to(device), y.to(device)\n        \n        with torch.amp.autocast(\"cuda\", enabled=CFG[\"mixed_precision\"]):\n            logits = model(x)\n            loss = criterion(logits, y)\n        \n        bs = x.size(0)\n        loss_sum += loss.item() * bs\n        acc_sum += (logits.argmax(1) == y).float().sum().item()\n        n += bs\n        \n        all_preds.append(logits.argmax(1).cpu().numpy())\n        all_trues.append(y.cpu().numpy())\n        \n    preds = np.concatenate(all_preds)\n    trues = np.concatenate(all_trues)\n    \n    f1 = f1_score(trues, preds, average=\"macro\")\n    cm = confusion_matrix(trues, preds, labels=list(range(num_classes)))\n    \n    return loss_sum / n, acc_sum / n, f1, cm, trues, preds\n\ndef save_metrics(cm, y_true, y_pred, class_names, out_dir: Path, epoch: int):\n    \"\"\"Saves the confusion matrix and a per-class classification report.\"\"\"\n    ensure_dir(out_dir)\n    \n    # Save confusion matrix\n    df_cm = pd.DataFrame(cm, columns=class_names, index=class_names)\n    df_cm.to_csv(out_dir / f\"confusion_matrix_epoch{epoch:03d}.csv\")\n\n    # Save classification report\n    report = classification_report(y_true, y_pred, labels=list(range(len(class_names))),\n                                   target_names=class_names, output_dict=True, zero_division=0)\n    pd.DataFrame(report).T.reset_index().rename(columns={\"index\": \"class\"}).to_csv(\n        out_dir / f\"per_class_report_epoch{epoch:03d}.csv\", index=False\n    )\n    \ndef save_curve(curves, out_csv: Path):\n    \"\"\"Saves the training history (loss, acc, f1) to a CSV file.\"\"\"\n    pd.DataFrame(curves).to_csv(out_csv, index=False)\n\n\ndef train_one_phase(model, train_loader, valid_loader, epochs, optimizer, criterion, \n                    curves, out_metrics_dir, class_names, start_epoch=1):\n    \"\"\"Main training loop for a single phase (e.g., linear probing or fine-tuning).\"\"\"\n    scaler = torch.amp.GradScaler(enabled=CFG[\"mixed_precision\"])\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max(1, epochs))\n    \n    best_f1 = -1\n    best_epoch = -1\n    best_path = out_metrics_dir.parent / f\"{CFG['run_name']}_best.pt\"\n    \n    num_total_epochs = start_epoch + epochs - 1\n    \n    for e in range(start_epoch, start_epoch + epochs):\n        model.train()\n        t0 = time.time()\n        loss_sum = acc_sum = n = 0\n        \n        for x, y, _ in train_loader:\n            x, y = x.to(device), y.to(device)\n            \n            optimizer.zero_grad(set_to_none=True)\n            \n            with torch.amp.autocast(\"cuda\", enabled=CFG[\"mixed_precision\"]):\n                logits = model(x)\n                loss = criterion(logits, y)\n            \n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n            scaler.step(optimizer)\n            scaler.update()\n            \n            bs = x.size(0)\n            loss_sum += loss.item() * bs\n            acc_sum += (logits.argmax(1) == y).float().sum().item()\n            n += bs\n            \n        scheduler.step()\n\n        # End of epoch evaluation\n        train_loss, train_acc = loss_sum / n, acc_sum / n\n        val_loss, val_acc, val_f1, cm, y_true, y_pred = evaluate(model, valid_loader, len(class_names))\n        \n        print(f\"[{CFG['run_name']}] Epoch {e:02d}/{num_total_epochs:02d} | \"\n              f\"Train Loss={train_loss:.4f}, Acc={train_acc:.4f} | \"\n              f\"Val Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f} | \"\n              f\"Time={int(time.time()-t0)}s\")\n        \n        # Log metrics\n        curves[\"epoch\"].append(e)\n        curves[\"train_loss\"].append(train_loss)\n        curves[\"train_acc\"].append(train_acc)\n        curves[\"val_loss\"].append(val_loss)\n        curves[\"val_acc\"].append(val_acc)\n        curves[\"val_f1\"].append(val_f1)\n        save_curve(curves, out_metrics_dir / \"train_curve.csv\")\n        save_metrics(cm, y_true, y_pred, class_names, out_metrics_dir, e)\n\n        # Save best model based on validation F1-score\n        if val_f1 > best_f1:\n            best_f1, best_epoch = val_f1, e\n            torch.save({\"state_dict\": model.state_dict(), \"classes\": class_names}, best_path)\n            print(f\"  -> New best model saved with F1-score: {best_f1:.4f}\")\n            \n    return best_f1, best_epoch, best_path","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================================================\n# Main Execution Block\n# =========================================================================================\n\n# 1. Set seed for reproducibility\nset_seed(CFG[\"seed\"])\n\n# 2. Prepare data splits and dataloaders\nroot = Path(CFG[\"data_root\"])\nclasses, train_items, val_items = build_seedlings_split(root, CFG[\"val_ratio\"], CFG[\"seed\"])\nnum_classes = len(classes)\nprint(f\"Found {num_classes} classes. Splitting into {len(train_items)} train and {len(val_items)} validation samples.\")\n\ntrain_tfm, valid_tfm = get_transforms(CFG[\"img_size\"])\ntrain_ds = PlantDataset(train_items, train_tfm)\nvalid_ds = PlantDataset(val_items, valid_tfm)\n\ntrain_loader = DataLoader(train_ds, batch_size=CFG[\"batch_size\"], shuffle=True,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_ds, batch_size=CFG[\"batch_size\"], shuffle=False,\n                          num_workers=CFG[\"num_workers\"], pin_memory=True)\n\n# 3. Setup output directories and run name\nif CFG[\"run_name\"] is None:\n    CFG[\"run_name\"] = f\"seedlings_{CFG['model_name']}_{CFG['strategy']}\"\nout_dir = Path(CFG[\"out_root\"]) / CFG[\"run_name\"]\nmetrics_dir = out_dir / \"metrics\"\nensure_dir(metrics_dir)\nprint(f\"Outputs will be saved to: {out_dir}\")\n\n# 4. Build model and load pre-trained backbone\nmodel = build_model(num_classes, CFG[\"model_name\"]).to(device)\nmodel = load_backbone_from_ckpt(model, CFG[\"ckpt_path\"], CFG[\"model_name\"])\n\n# 5. Save run configuration\nwith open(metrics_dir / \"config.json\", \"w\") as f:\n    json.dump(CFG, f, indent=4)\n\n# 6. Initialize for training\ncurves = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"val_f1\": []}\ncriterion = nn.CrossEntropyLoss(label_smoothing=CFG[\"label_smoothing\"])\nbest_f1_overall, best_epoch_overall = -1, -1\n\n# 7. Execute training based on the selected strategy\n# ---------------------------------------------------\nif CFG[\"strategy\"] == \"full_ft\":\n    print(\"\\n--- Starting Training: Full Fine-Tuning Strategy ---\")\n    set_backbone_trainable(model, CFG[\"model_name\"], trainable=True)\n    optimizer = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                               wd=CFG[\"weight_decay\"], full_ft=True)\n    best_f1_overall, best_epoch_overall, _ = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_fullft\"], optimizer, criterion, \n        curves, metrics_dir, classes\n    )\n\nelif CFG[\"strategy\"] == \"lp_unfreeze\":\n    # Phase 1: Linear Probing (train only the head)\n    print(\"\\n--- Starting Training Phase 1: Linear Probing ---\")\n    set_backbone_trainable(model, CFG[\"model_name\"], trainable=False)\n    optimizer = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                               wd=CFG[\"weight_decay\"], full_ft=False)\n    best_f1_overall, best_epoch_overall, _ = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_lp\"], optimizer, criterion, \n        curves, metrics_dir, classes, start_epoch=1\n    )\n\n    # Phase 2: Gradual Unfreezing (fine-tune deeper layers)\n    print(\"\\n--- Starting Training Phase 2: Gradual Unfreezing ---\")\n    stages = 1 if CFG[\"model_name\"] == \"convnext_tiny\" else 2\n    unfreeze_last_stages(model, CFG[\"model_name\"], stages_to_unfreeze=stages)\n    optimizer = make_optimizer(model, CFG[\"model_name\"], CFG[\"lr_backbone\"], CFG[\"lr_head\"],\n                               wd=CFG[\"weight_decay\"], full_ft=True)\n    f1_p2, ep_p2, _ = train_one_phase(\n        model, train_loader, valid_loader, CFG[\"epochs_unfreeze\"], optimizer, criterion, \n        curves, metrics_dir, classes, start_epoch=(CFG[\"epochs_lp\"] + 1)\n    )\n    if f1_p2 > best_f1_overall:\n        best_f1_overall, best_epoch_overall = f1_p2, ep_p2\n\nelse:\n    raise ValueError(\"CFG['strategy'] must be either 'full_ft' or 'lp_unfreeze'\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training Finished!\")\nprint(f\"Best overall macro-F1 score: {best_f1_overall:.4f} achieved at epoch {best_epoch_overall}.\")\nprint(f\"All outputs, metrics, and best model are saved in: {out_dir}\")\nprint(\"=\"*50)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}